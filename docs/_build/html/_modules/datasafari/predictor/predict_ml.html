<!doctype html>
<html class="no-js" lang="en" data-content_root="../../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="../../../genindex.html" /><link rel="search" title="Search" href="../../../search.html" />

    <link rel="shortcut icon" href="../../../_static/favicon.ico"/><!-- Generated with Sphinx 7.3.7 and Furo 2024.05.06 -->
        <title>datasafari.predictor.predict_ml - DataSafari 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo.css?v=387cc868" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" /
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../../index.html"><div class="brand">DataSafari 1.0.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon no-toc" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../../../_static/logos/ds-branding-logo-big-lightmode.png" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../../../_static/logos/ds-branding-logo-big-darkmode.png" alt="Dark Logo"/>
  </div>
  
  <span class="sidebar-brand-text">DataSafari 1.0.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glance.html">Subpackages Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Explorers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.explorer.explore_df.html">explore_df()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.explorer.explore_num.html">explore_num()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.explorer.explore_cat.html">explore_cat()</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.transformer.transform_num.html">transform_num()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.transformer.transform_cat.html">transform_cat()</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Evaluators</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.evaluator.evaluate_normality.html">evaluate_normality()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.evaluator.evaluate_variance.html">evaluate_variance()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.evaluator.evaluate_dtype.html">evaluate_dtype()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.evaluator.evaluate_contingency_table.html">evaluate_contingency_table()</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Predictors</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.predictor.predict_hypothesis.html">predict_hypothesis()</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasafari.predictor.predict_ml.html">predict_ml()</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../genindex.html">General Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../lic-gpl3.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contact.html">Contact</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon no-toc" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <h1>Source code for datasafari.predictor.predict_ml</h1><div class="highlight"><pre>
<span></span><span class="c1"># used overall:</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># mostly used within data_preprocessing_core():</span>
<span class="kn">from</span> <span class="nn">datasafari.evaluator.evaluate_dtype</span> <span class="kn">import</span> <span class="n">evaluate_dtype</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">TransformerMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_validate</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">FunctionTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="c1"># mostly used within model_recommendation_core():</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span><span class="p">,</span> <span class="n">SVR</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span><span class="p">,</span> <span class="n">KNeighborsRegressor</span>
<span class="c1"># mostly used within the model_inference_core():</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="c1"># mostly used within the model_tuning_core():</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">skopt</span> <span class="kn">import</span> <span class="n">BayesSearchCV</span>


<span class="c1"># Meta data #</span>

<span class="c1"># Available Classification Models</span>
<span class="n">models_classification</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;LogisticRegression&#39;</span><span class="p">:</span> <span class="n">LogisticRegression</span><span class="p">(),</span>
    <span class="s1">&#39;DecisionTreeClassifier&#39;</span><span class="p">:</span> <span class="n">DecisionTreeClassifier</span><span class="p">(),</span>
    <span class="s1">&#39;RandomForestClassifier&#39;</span><span class="p">:</span> <span class="n">RandomForestClassifier</span><span class="p">(),</span>
    <span class="s1">&#39;GradientBoostingClassifier&#39;</span><span class="p">:</span> <span class="n">GradientBoostingClassifier</span><span class="p">(),</span>
    <span class="s1">&#39;SVC&#39;</span><span class="p">:</span> <span class="n">SVC</span><span class="p">(</span><span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="s1">&#39;KNeighborsClassifier&#39;</span><span class="p">:</span> <span class="n">KNeighborsClassifier</span><span class="p">(),</span>
<span class="p">}</span>

<span class="c1"># Available Regression Models</span>
<span class="n">models_regression</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;LinearRegression&#39;</span><span class="p">:</span> <span class="n">LinearRegression</span><span class="p">(),</span>
    <span class="s1">&#39;Ridge&#39;</span><span class="p">:</span> <span class="n">Ridge</span><span class="p">(),</span>
    <span class="s1">&#39;Lasso&#39;</span><span class="p">:</span> <span class="n">Lasso</span><span class="p">(),</span>
    <span class="s1">&#39;DecisionTreeRegressor&#39;</span><span class="p">:</span> <span class="n">DecisionTreeRegressor</span><span class="p">(),</span>
    <span class="s1">&#39;RandomForestRegressor&#39;</span><span class="p">:</span> <span class="n">RandomForestRegressor</span><span class="p">(),</span>
    <span class="s1">&#39;GradientBoostingRegressor&#39;</span><span class="p">:</span> <span class="n">GradientBoostingRegressor</span><span class="p">(),</span>
    <span class="s1">&#39;SVR&#39;</span><span class="p">:</span> <span class="n">SVR</span><span class="p">(),</span>
    <span class="s1">&#39;KNeighborsRegressor&#39;</span><span class="p">:</span> <span class="n">KNeighborsRegressor</span><span class="p">(),</span>
<span class="p">}</span>

<span class="c1"># Available Classification Models (inference)</span>
<span class="n">models_classification_inference</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Logit&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">logit</span><span class="p">,</span>  <span class="c1"># Logistic Regression</span>
    <span class="s1">&#39;Probit&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">probit</span><span class="p">,</span>  <span class="c1"># Probit Regression</span>
    <span class="s1">&#39;MNLogit&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">mnlogit</span><span class="p">,</span>  <span class="c1"># Multinomial Logistic Regression</span>
    <span class="s1">&#39;Poisson&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">poisson</span><span class="p">,</span>  <span class="c1"># Poisson Regression for count data</span>
    <span class="s1">&#39;NegativeBinomial&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">negativebinomial</span><span class="p">,</span>  <span class="c1"># Negative Binomial Regression for over-dispersed count data</span>
    <span class="s1">&#39;GEE&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">gee</span><span class="p">,</span>  <span class="c1"># Generalized Estimating Equations for repeated measurements</span>
    <span class="s1">&#39;NominalGEE&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">nominal_gee</span><span class="p">,</span>  <span class="c1"># GEE for nominal response</span>
    <span class="s1">&#39;OrdinalGEE&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">ordinal_gee</span>  <span class="c1"># GEE for ordinal response</span>
<span class="p">}</span>

<span class="c1"># Available Regression Models (inference)</span>
<span class="n">models_regression_inference</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;OLS&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">,</span>  <span class="c1"># Ordinary Least Squares Linear Regression</span>
    <span class="s1">&#39;WLS&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">wls</span><span class="p">,</span>  <span class="c1"># Weighted Least Squares Linear Regression</span>
    <span class="s1">&#39;GLS&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">gls</span><span class="p">,</span>  <span class="c1"># Generalized Least Squares Linear Regression</span>
    <span class="s1">&#39;RLM&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">rlm</span><span class="p">,</span>  <span class="c1"># Robust Linear Models</span>
    <span class="s1">&#39;QuantReg&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">quantreg</span><span class="p">,</span>  <span class="c1"># Quantile Regression</span>
    <span class="s1">&#39;GLSAR&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">glsar</span><span class="p">,</span>  <span class="c1"># GLS with autoregressive errors</span>
    <span class="s1">&#39;MixedLM&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">mixedlm</span><span class="p">,</span>  <span class="c1"># Mixed Linear Model for hierarchical or longitudinal data</span>
    <span class="s1">&#39;PHReg&#39;</span><span class="p">:</span> <span class="n">smf</span><span class="o">.</span><span class="n">phreg</span>  <span class="c1"># Proportional Hazards Regression for survival analysis</span>
<span class="p">}</span>

<span class="c1"># Available Scoring Metrics for Classification using Scikit</span>
<span class="n">scoring_classification</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Accuracy&#39;</span><span class="p">:</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Balanced Accuracy&#39;</span><span class="p">:</span> <span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Average Precision&#39;</span><span class="p">:</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Neg Brier Score&#39;</span><span class="p">:</span> <span class="s1">&#39;neg_brier_score&#39;</span><span class="p">,</span>
    <span class="s1">&#39;F1 (Micro)&#39;</span><span class="p">:</span> <span class="s1">&#39;f1_micro&#39;</span><span class="p">,</span>
    <span class="s1">&#39;F1 (Macro)&#39;</span><span class="p">:</span> <span class="s1">&#39;f1_macro&#39;</span><span class="p">,</span>
    <span class="s1">&#39;F1 (Weighted)&#39;</span><span class="p">:</span> <span class="s1">&#39;f1_weighted&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Neg Log Loss&#39;</span><span class="p">:</span> <span class="s1">&#39;neg_log_loss&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Precision (Micro)&#39;</span><span class="p">:</span> <span class="s1">&#39;precision_micro&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Precision (Macro)&#39;</span><span class="p">:</span> <span class="s1">&#39;precision_macro&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Precision (Weighted)&#39;</span><span class="p">:</span> <span class="s1">&#39;precision_weighted&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Recall (Micro)&#39;</span><span class="p">:</span> <span class="s1">&#39;recall_micro&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Recall (Macro)&#39;</span><span class="p">:</span> <span class="s1">&#39;recall_macro&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Recall (Weighted)&#39;</span><span class="p">:</span> <span class="s1">&#39;recall_weighted&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Jaccard (Micro)&#39;</span><span class="p">:</span> <span class="s1">&#39;jaccard_micro&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Jaccard (Macro)&#39;</span><span class="p">:</span> <span class="s1">&#39;jaccard_macro&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Jaccard (Weighted)&#39;</span><span class="p">:</span> <span class="s1">&#39;jaccard_weighted&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ROC AUC (OVR)&#39;</span><span class="p">:</span> <span class="s1">&#39;roc_auc_ovr&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ROC AUC (OVO)&#39;</span><span class="p">:</span> <span class="s1">&#39;roc_auc_ovo&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Available Scoring Metrics for Regression using Scikit</span>
<span class="n">scoring_regression</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;EV&#39;</span><span class="p">:</span> <span class="s1">&#39;explained_variance&#39;</span><span class="p">,</span>
    <span class="s1">&#39;MAE&#39;</span><span class="p">:</span> <span class="s1">&#39;neg_mean_absolute_error&#39;</span><span class="p">,</span>
    <span class="s1">&#39;MSE&#39;</span><span class="p">:</span> <span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span> <span class="s1">&#39;neg_root_mean_squared_error&#39;</span><span class="p">,</span>
    <span class="s1">&#39;MSLE&#39;</span><span class="p">:</span> <span class="s1">&#39;neg_mean_squared_log_error&#39;</span><span class="p">,</span>
    <span class="s1">&#39;MedAE&#39;</span><span class="p">:</span> <span class="s1">&#39;neg_median_absolute_error&#39;</span><span class="p">,</span>
    <span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="s1">&#39;r2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;MPD&#39;</span><span class="p">:</span> <span class="s1">&#39;neg_mean_poisson_deviance&#39;</span><span class="p">,</span>
    <span class="s1">&#39;MGD&#39;</span><span class="p">:</span> <span class="s1">&#39;neg_mean_gamma_deviance&#39;</span><span class="p">,</span>
    <span class="s1">&#39;MAPE&#39;</span><span class="p">:</span> <span class="s1">&#39;neg_mean_absolute_percentage_error&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Available Inference Metrics for Regression using Statsmodels</span>
<span class="n">scoring_regression_inference</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;AIC&#39;</span><span class="p">:</span> <span class="s1">&#39;aic&#39;</span><span class="p">,</span>
    <span class="s1">&#39;BIC&#39;</span><span class="p">:</span> <span class="s1">&#39;bic&#39;</span><span class="p">,</span>
    <span class="s1">&#39;R-squared&#39;</span><span class="p">:</span> <span class="s1">&#39;rsquared&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Adjusted R-squared&#39;</span><span class="p">:</span> <span class="s1">&#39;rsquared_adj&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Log-Likelihood&#39;</span><span class="p">:</span> <span class="s1">&#39;llf&#39;</span>
<span class="p">}</span>

<span class="c1"># Available Inference Metrics for Classification using Statsmodels</span>
<span class="n">scoring_classification_inference</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;AIC&#39;</span><span class="p">:</span> <span class="s1">&#39;aic&#39;</span><span class="p">,</span>
    <span class="s1">&#39;BIC&#39;</span><span class="p">:</span> <span class="s1">&#39;bic&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Pseudo R-squared&#39;</span><span class="p">:</span> <span class="s1">&#39;prsquared&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Log-Likelihood&#39;</span><span class="p">:</span> <span class="s1">&#39;llf&#39;</span>
<span class="p">}</span>

<span class="c1"># Tips for Classification Scoring Metrics (if verbose &gt; 1) - idea is to aid interpretation for non-advanced users</span>
<span class="n">tips_scoring_classification</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Accuracy&#39;</span><span class="p">:</span> <span class="s2">&quot;Overall correctness, suitable for balanced classes. Higher scores indicate better performance.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Balanced Accuracy&#39;</span><span class="p">:</span> <span class="s2">&quot;Accuracy per class, great for imbalanced data. Higher values signal balanced class prediction capability.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Average Precision&#39;</span><span class="p">:</span> <span class="s2">&quot;Precision-recall balance, ideal for ranking tasks. Higher scores suggest better model precision.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Neg Brier Score&#39;</span><span class="p">:</span> <span class="s2">&quot;Probability calibration, lower is better, indicating accurate confidence in predictions.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;F1 (Micro)&#39;</span><span class="p">:</span> <span class="s2">&quot;Aggregated F1 score, good for unbalanced data. High score shows effective overall class prediction.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;F1 (Macro)&#39;</span><span class="p">:</span> <span class="s2">&quot;Mean F1 score across classes, for equal class emphasis. High values mean balanced performance across classes.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;F1 (Weighted)&#39;</span><span class="p">:</span> <span class="s2">&quot;F1 score weighted by class, for imbalanced data. Reflects performance weighted towards prevalent classes.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Neg Log Loss&#39;</span><span class="p">:</span> <span class="s2">&quot;Model confidence, lower scores show better probability estimates.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Precision (Micro)&#39;</span><span class="p">:</span> <span class="s2">&quot;Overall model precision, useful in multiclass settings. High values indicate fewer false positives.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Precision (Macro)&#39;</span><span class="p">:</span> <span class="s2">&quot;Average precision, highlights class-specific performance. A high score denotes effective class differentiation.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Precision (Weighted)&#39;</span><span class="p">:</span> <span class="s2">&quot;Precision accounting for class imbalance. Focuses precision assessment on more frequent classes.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Recall (Micro)&#39;</span><span class="p">:</span> <span class="s2">&quot;Overall true positive rate. High scores show effectiveness in identifying positive instances.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Recall (Macro)&#39;</span><span class="p">:</span> <span class="s2">&quot;Balanced true positive rate, useful for equal class focus. Reflects consistent recall across classes.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Recall (Weighted)&#39;</span><span class="p">:</span> <span class="s2">&quot;Recall adjusted for class size, emphasizing larger classes. Indicates model&#39;s effectiveness on common classes.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Jaccard (Micro)&#39;</span><span class="p">:</span> <span class="s2">&quot;Intersection over union, measured globally. High values indicate broad prediction alignment with truth.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Jaccard (Macro)&#39;</span><span class="p">:</span> <span class="s2">&quot;Average IoU for each class, shows class-wise model agreement. High score signifies precise class predictions.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;Jaccard (Weighted)&#39;</span><span class="p">:</span> <span class="s2">&quot;IoU weighted by class frequency. Targets performance improvement in dominant classes.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;ROC AUC (OVR)&#39;</span><span class="p">:</span> <span class="s2">&quot;Area under ROC for multiclass, one-vs-rest. Higher scores mean better distinction between classes.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;ROC AUC (OVO)&#39;</span><span class="p">:</span> <span class="s2">&quot;Area under ROC for multiclass, one-vs-one. Indicates model&#39;s discriminative power between any two classes.&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Tips for Regression Scoring Metrics</span>
<span class="n">tips_scoring_regression</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;EV&#39;</span><span class="p">:</span> <span class="s2">&quot;Explains variance, perfect for models aiming high explanation power. Closer to 1 indicates better model.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;MaxError&#39;</span><span class="p">:</span> <span class="s2">&quot;Worst-case error, critical for risk-sensitive models. Lower values denote reliability.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;MAE&#39;</span><span class="p">:</span> <span class="s2">&quot;Average error magnitude, less sensitive to outliers. Lower MAE suggests higher precision.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;MSE&#39;</span><span class="p">:</span> <span class="s2">&quot;Penalizes larger errors more, great for models where large errors are especially undesirable. Lower is better.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span> <span class="s2">&quot;Square root of MSE, on the target scale. Lower values indicate fewer and smaller errors.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;MSLE&#39;</span><span class="p">:</span> <span class="s2">&quot;Focuses on relative errors, ideal for growth predictions. Lower scores reflect better accuracy on percentage scale.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;MedAE&#39;</span><span class="p">:</span> <span class="s2">&quot;Middle error value, robust to outliers. Useful for skewed data, lower MedAE indicates central tendency accuracy.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="s2">&quot;Proportion of variance explained, best for predictive models. Closer to 1, the more explanatory the model.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;MPD&#39;</span><span class="p">:</span> <span class="s2">&quot;For count data, penalizing under/overestimations differently. Lower scores indicate Poisson conformity.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;MGD&#39;</span><span class="p">:</span> <span class="s2">&quot;Assesses fit for gamma-distributed outcomes. Lower values show better adherence to gamma distribution.&quot;</span><span class="p">,</span>
    <span class="s1">&#39;MAPE&#39;</span><span class="p">:</span> <span class="s2">&quot;Percentage error, useful for comparative error measurement. Lower MAPE indicates better relative accuracy.&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Available Tuners</span>
<span class="n">tuners</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;grid&#39;</span><span class="p">:</span> <span class="n">GridSearchCV</span><span class="p">,</span>
    <span class="s1">&#39;random&#39;</span><span class="p">:</span> <span class="n">RandomizedSearchCV</span><span class="p">,</span>
    <span class="s1">&#39;bayesian&#39;</span><span class="p">:</span> <span class="n">BayesSearchCV</span>
<span class="p">}</span>

<span class="c1"># User-friendly tuner names for outputs</span>
<span class="n">tuner_names</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;grid&#39;</span><span class="p">:</span> <span class="s1">&#39;GridSearchCV&#39;</span><span class="p">,</span>
    <span class="s1">&#39;random&#39;</span><span class="p">:</span> <span class="s1">&#39;RandomizedSearchCV&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bayesian&#39;</span><span class="p">:</span> <span class="s1">&#39;BayesSearchCV&#39;</span>
<span class="p">}</span>

<span class="c1"># Default Parameter Grids for Classification Models</span>
<span class="n">default_param_grids_classification</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;LogisticRegression&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
        <span class="s1">&#39;penalty&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;l2&#39;</span><span class="p">],</span>
        <span class="s1">&#39;solver&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;newton-cg&#39;</span><span class="p">,</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">,</span> <span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;DecisionTreeClassifier&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
        <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;RandomForestClassifier&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">],</span>
        <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="s1">&#39;sqrt&#39;</span><span class="p">],</span>
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span>
        <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;GradientBoostingClassifier&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;SVC&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span>
        <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">],</span>
        <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;auto&#39;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;KNeighborsClassifier&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
        <span class="s1">&#39;weights&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;distance&#39;</span><span class="p">],</span>
        <span class="s1">&#39;algorithm&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="s1">&#39;ball_tree&#39;</span><span class="p">,</span> <span class="s1">&#39;kd_tree&#39;</span><span class="p">,</span> <span class="s1">&#39;brute&#39;</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1"># Default Parameter Grids for Regression Models</span>
<span class="n">default_param_grids_regression</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;LinearRegression&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="c1"># Linear Regression usually does not need hyperparameter tuning except for regularization</span>
    <span class="p">},</span>
    <span class="s1">&#39;Ridge&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">],</span>
        <span class="s1">&#39;solver&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="s1">&#39;svd&#39;</span><span class="p">,</span> <span class="s1">&#39;cholesky&#39;</span><span class="p">,</span> <span class="s1">&#39;lsqr&#39;</span><span class="p">,</span> <span class="s1">&#39;sparse_cg&#39;</span><span class="p">,</span> <span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;Lasso&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">],</span>
        <span class="s1">&#39;selection&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;cyclic&#39;</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;DecisionTreeRegressor&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
        <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;RandomForestRegressor&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">],</span>
        <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="s1">&#39;sqrt&#39;</span><span class="p">],</span>
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span>
        <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;GradientBoostingRegressor&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
        <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;SVR&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span>
        <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">],</span>
        <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;auto&#39;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="s1">&#39;KNeighborsRegressor&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
        <span class="s1">&#39;weights&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;distance&#39;</span><span class="p">],</span>
        <span class="s1">&#39;algorithm&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="s1">&#39;ball_tree&#39;</span><span class="p">,</span> <span class="s1">&#39;kd_tree&#39;</span><span class="p">,</span> <span class="s1">&#39;brute&#39;</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">}</span>


<span class="c1"># Utility Functions (predict_ml() back-end) #</span>
<span class="k">def</span> <span class="nf">datetime_feature_extractor</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="n">feature_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="c1"># Ensure the column is in datetime format</span>
        <span class="n">datetime_series</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">])</span>
        <span class="c1"># Extract datetime features</span>
        <span class="n">feature_df</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s2">_year&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">datetime_series</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span>
        <span class="n">feature_df</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s2">_month&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">datetime_series</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">month</span>
        <span class="n">feature_df</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s2">_day&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">datetime_series</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">day</span>
    <span class="k">return</span> <span class="n">feature_df</span>


<span class="k">def</span> <span class="nf">data_preprocessing_core</span><span class="p">(</span>
        <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
        <span class="n">x_cols</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">y_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">data_state</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;unprocessed&#39;</span><span class="p">,</span>
        <span class="n">test_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span>
        <span class="n">numeric_imputer</span><span class="p">:</span> <span class="n">TransformerMixin</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;median&#39;</span><span class="p">),</span>
        <span class="n">numeric_scaler</span><span class="p">:</span> <span class="n">TransformerMixin</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">(),</span>
        <span class="n">categorical_imputer</span><span class="p">:</span> <span class="n">TransformerMixin</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="s1">&#39;missing&#39;</span><span class="p">),</span>
        <span class="n">categorical_encoder</span><span class="p">:</span> <span class="n">TransformerMixin</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">),</span>
        <span class="n">text_vectorizer</span><span class="p">:</span> <span class="n">TransformerMixin</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(),</span>
        <span class="n">datetime_transformer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">],</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]</span> <span class="o">=</span> <span class="n">FunctionTransformer</span><span class="p">(</span><span class="n">datetime_feature_extractor</span><span class="p">,</span> <span class="n">validate</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Performs comprehensive preprocessing on a dataset containing mixed data types.**</span>

<span class="sd">    This function prepares a dataset for machine learning by handling numerical, categorical, text, and datetime data. It supports flexible imputation, scaling, encoding, and vectorization methods to cater to a wide range of preprocessing needs. The function automatically splits the data into training and test sets and applies the preprocessing steps defined by the user. It accommodates custom preprocessing steps for various data types, enhancing flexibility and control over the preprocessing pipeline.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    df : pd.DataFrame</span>
<span class="sd">        The DataFrame to preprocess.</span>

<span class="sd">    x_cols : list of str</span>
<span class="sd">        List of feature column names in `df` to include in the preprocessing.</span>

<span class="sd">    y_col : str</span>
<span class="sd">        The name of the target variable column in `df`.</span>

<span class="sd">    data_state : str, optional, default: &#39;unprocessed&#39;</span>
<span class="sd">        Specifies the initial state of the data (&#39;unprocessed&#39; or &#39;preprocessed&#39;).</span>

<span class="sd">    test_size : float, optional, default: 0.2</span>
<span class="sd">        Proportion of the dataset to include in the test split.</span>

<span class="sd">    random_state : int, optional, default: 42</span>
<span class="sd">        Controls the shuffling applied to the data before applying the split.</span>

<span class="sd">    numeric_imputer : sklearn imputer object, optional, default: SimpleImputer(strategy=&#39;median&#39;)</span>
<span class="sd">        The imputation transformer for handling missing values in numerical data.</span>

<span class="sd">    numeric_scaler : sklearn scaler object, optional, default: StandardScaler()</span>
<span class="sd">        The scaling transformer for numerical data.</span>

<span class="sd">    categorical_imputer : sklearn imputer object, optional, default: SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;missing&#39;)</span>
<span class="sd">        The imputation transformer for handling missing values in categorical data.</span>

<span class="sd">    categorical_encoder : sklearn encoder object, optional, default: OneHotEncoder(handle_unknown=&#39;ignore&#39;)</span>
<span class="sd">        The encoding transformer for categorical data.</span>

<span class="sd">    text_vectorizer : sklearn vectorizer object, optional, default: CountVectorizer()</span>
<span class="sd">        The vectorization transformer for text data.</span>

<span class="sd">    datetime_transformer : callable, optional, default: None</span>
<span class="sd">        Transformer for datetime data.</span>

<span class="sd">            *Note: This parameter defaults to a custom datetime transformer, which extracts year, month, and day as separate features.*</span>

<span class="sd">    verbose : int, optional, default: 1</span>
<span class="sd">        The higher value the more output and information the user receives.</span>


<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    x_train_processed : ndarray</span>
<span class="sd">        The preprocessed training feature set. *Note: The x_train will be untouched if ``data_state=&#39;preprocessed&#39;``*</span>

<span class="sd">    x_test_processed : ndarray</span>
<span class="sd">        The preprocessed test feature set. *Note: The x_train will be untouched if ``data_state=&#39;preprocessed&#39;``*</span>

<span class="sd">    y_train : Series</span>
<span class="sd">        The training target variable.</span>

<span class="sd">    y_test : Series</span>
<span class="sd">        The test target variable.</span>

<span class="sd">    task_type : str</span>
<span class="sd">        The type of machine learning task inferred from the target variable (&#39;regression&#39; or &#39;classification&#39;).</span>


<span class="sd">    Raises:</span>
<span class="sd">    -------</span>
<span class="sd">    TypeErrors:</span>
<span class="sd">        - If &#39;df&#39; is not a pandas DataFrame.</span>
<span class="sd">        - If &#39;x_cols&#39; is not a list of strings.</span>
<span class="sd">        - If &#39;y_col&#39; is not a string.</span>
<span class="sd">        - If &#39;data_state&#39; is not a string.</span>
<span class="sd">        - If &#39;test_size&#39; is not a float.</span>
<span class="sd">        - If &#39;random_state&#39; is not an integer.</span>
<span class="sd">        - If &#39;verbose&#39; is not an integer</span>
<span class="sd">        - If numeric_imputer, numeric_scaler, categorical_imputer, categorical_encoder, text_vectorizer, or datetime_transformer do not support the required interface.</span>
<span class="sd">    ValueErrors:</span>
<span class="sd">        - If the `df` is empty.</span>
<span class="sd">        - If &#39;data_state&#39; is not &#39;unprocessed&#39; or &#39;preprocessed&#39;.</span>
<span class="sd">        - If &#39;y_col&#39; is not found in &#39;df&#39;.</span>
<span class="sd">        - If specified &#39;x_cols&#39; are not present in &#39;df&#39;.</span>
<span class="sd">        - If &#39;test_size&#39; is not between 0 and 1.</span>
<span class="sd">        - If &#39;df&#39; does not contain enough data to split according to &#39;test_size&#39;.</span>


<span class="sd">    Notes:</span>
<span class="sd">    ------</span>
<span class="sd">    The ``data_preprocessing_core()`` function is an integral part of the ``predict_ml()`` pipeline, designed to automate the preprocessing steps for machine learning tasks. It handles various data types, including numerical, categorical, text, and datetime, providing a streamlined process for preparing data for model training. This function uses Scikit-learn&#39;s transformers and custom functions to perform imputation, scaling, encoding, and vectorization, allowing users to customize these steps according to their needs.</span>

<span class="sd">    This function supports flexible preprocessing, accommodating custom transformations through its parameters. By specifying transformers for different data types, users can adapt the preprocessing to fit their dataset&#39;s specific characteristics. The function also splits the data into training and test sets, facilitating model evaluation and selection later in the pipeline.</span>

<span class="sd">    Designed with usability in mind, ``data_preprocessing_core()`` includes console output options controlled by &#39;verbose&#39; parameter. This feature provides users with insights into the preprocessing steps taken (verbose &gt; 0), including information on processed features and tips for further customization (verbose &gt; 1).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">datetime_transformer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">datetime_transformer</span> <span class="o">=</span> <span class="n">FunctionTransformer</span><span class="p">(</span><span class="n">datetime_feature_extractor</span><span class="p">,</span> <span class="n">validate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Error Handling</span>
    <span class="c1"># TypeErrors</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;df&#39; parameter must be a pandas DataFrame.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x_cols</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">x_cols</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;x_cols&#39; parameter must be a list of strings representing column names.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y_col</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;y_col&#39; parameter must be a string representing the target column name.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_state</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;data_state&#39; parameter must be a string and one of [&#39;unprocessed&#39;, &#39;preprocessed&#39;].&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">test_size</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;test_size&#39; parameter must be a float.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;random_state&#39; parameter must be an integer.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">verbose</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): &#39;verbose&#39; must be an integer value.&quot;</span><span class="p">)</span>

    <span class="c1"># Checking for the essential methods in imputers, scalers, encoders, and vectorizers</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">numeric_imputer</span><span class="p">,</span> <span class="s1">&#39;fit_transform&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">numeric_imputer</span><span class="p">,</span> <span class="s1">&#39;fit&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">numeric_imputer</span><span class="p">,</span> <span class="s1">&#39;transform&#39;</span><span class="p">))):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;numeric_imputer&#39; must support &#39;fit_transform&#39; or both &#39;fit&#39; and &#39;transform&#39; methods.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">numeric_scaler</span><span class="p">,</span> <span class="s1">&#39;fit_transform&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;numeric_scaler&#39; must support &#39;fit_transform&#39; method.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">categorical_imputer</span><span class="p">,</span> <span class="s1">&#39;fit_transform&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">categorical_imputer</span><span class="p">,</span> <span class="s1">&#39;fit&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">categorical_imputer</span><span class="p">,</span> <span class="s1">&#39;transform&#39;</span><span class="p">))):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;categorical_imputer&#39; must support &#39;fit_transform&#39; or both &#39;fit&#39; and &#39;transform&#39; methods.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">categorical_encoder</span><span class="p">,</span> <span class="s1">&#39;fit_transform&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;categorical_encoder&#39; must support &#39;fit_transform&#39; method.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">text_vectorizer</span><span class="p">,</span> <span class="s1">&#39;fit_transform&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;text_vectorizer&#39; must support &#39;fit_transform&#39; method.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">datetime_transformer</span><span class="p">,</span> <span class="s1">&#39;fit_transform&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;datetime_transformer&#39; must be callable or support a &#39;transform&#39; method.&quot;</span><span class="p">)</span>

    <span class="c1"># ValueErrors</span>
    <span class="k">if</span> <span class="n">df</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;explore_num(): The input DataFrame is empty.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">test_size</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The &#39;test_size&#39; parameter must be a float between 0 and 1.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">data_state</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;unprocessed&#39;</span><span class="p">,</span> <span class="s1">&#39;preprocessed&#39;</span><span class="p">]</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;data_preprocessing_core(): The data_state must be one of the following: </span><span class="se">\n</span><span class="s2">- &#39;unprocessed&#39;: activates predict_ml() preprocessing capabilities.</span><span class="se">\n</span><span class="s2"> - &#39;preprocessed&#39;: user opts out of preprocessing (Warning: ensure your data is properly preprocessed for ML)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">y_col</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;data_preprocessing_core(): The specified target column &#39;</span><span class="si">{</span><span class="n">y_col</span><span class="si">}</span><span class="s2">&#39; is not present in the DataFrame.&quot;</span><span class="p">)</span>

    <span class="n">missing_cols</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">x_cols</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">missing_cols</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;data_preprocessing_core(): The following feature columns are not present in the DataFrame: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">missing_cols</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Main Functionality</span>
    <span class="c1"># split data</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">x_cols</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="n">y_col</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

    <span class="c1"># define task type based on the target variable data type</span>
    <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">evaluate_dtype</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="p">[</span><span class="n">y_col</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="s1">&#39;dict&#39;</span><span class="p">)[</span><span class="n">y_col</span><span class="p">]</span>
    <span class="n">task_type</span> <span class="o">=</span> <span class="s1">&#39;regression&#39;</span> <span class="k">if</span> <span class="n">y_dtype</span> <span class="o">==</span> <span class="s1">&#39;numerical&#39;</span> <span class="k">else</span> <span class="s1">&#39;classification&#39;</span>

    <span class="k">if</span> <span class="n">data_state</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;unprocessed&#39;</span><span class="p">:</span>
        <span class="c1"># evaluate data types to determine preprocessing needs</span>
        <span class="n">data_types</span> <span class="o">=</span> <span class="n">evaluate_dtype</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x_cols</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="s1">&#39;dict&#39;</span><span class="p">)</span>

        <span class="n">numeric_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">data_types</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;numerical&#39;</span><span class="p">]</span>
        <span class="n">categorical_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">data_types</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;categorical&#39;</span><span class="p">]</span>
        <span class="n">text_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">data_types</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;text&#39;</span><span class="p">]</span>
        <span class="n">datetime_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">data_types</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;datetime&#39;</span><span class="p">]</span>

        <span class="c1"># preprocessing for numerical columns</span>
        <span class="n">numeric_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">numeric_imputer</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">numeric_scaler</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="c1"># preprocessing for categorical columns</span>
        <span class="n">categorical_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">categorical_imputer</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;encoder&#39;</span><span class="p">,</span> <span class="n">categorical_encoder</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="c1"># preprocessing for text columns</span>
        <span class="n">text_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;vectorizer&#39;</span><span class="p">,</span> <span class="n">text_vectorizer</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="c1"># combine preprocessing steps</span>
        <span class="n">preprocessor</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span><span class="n">transformers</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="n">numeric_transformer</span><span class="p">,</span> <span class="n">numeric_features</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">categorical_transformer</span><span class="p">,</span> <span class="n">categorical_features</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="n">text_transformer</span><span class="p">,</span> <span class="n">text_features</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;datetime&#39;</span><span class="p">,</span> <span class="n">datetime_transformer</span><span class="p">,</span> <span class="n">datetime_features</span><span class="p">)</span>
        <span class="p">],</span> <span class="n">remainder</span><span class="o">=</span><span class="s1">&#39;passthrough&#39;</span><span class="p">)</span>  <span class="c1"># handle columns not specified in transformers</span>

        <span class="c1"># apply preprocessing</span>
        <span class="n">x_train_processed</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
        <span class="n">x_test_processed</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

        <span class="c1"># define transformer names for reporting</span>
        <span class="n">numeric_processor_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">numeric_imputer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot; &amp; &quot;</span> <span class="o">+</span> <span class="nb">type</span><span class="p">(</span><span class="n">numeric_scaler</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">categorical_processor_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">categorical_imputer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot; &amp; &quot;</span> <span class="o">+</span> <span class="nb">type</span><span class="p">(</span><span class="n">categorical_encoder</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">text_processor_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">text_vectorizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">datetime_processor_name</span> <span class="o">=</span> <span class="s2">&quot;Custom DateTime Processing&quot;</span>

        <span class="c1"># construct console output</span>
        <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&lt; PREPROCESSING DATA REPORT &gt;&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Tip: You can define your own SciKit preprocessors using the appropriate parameters, please refer to documentation. </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Numerical features processed [using </span><span class="si">{</span><span class="n">numeric_processor_name</span><span class="si">}</span><span class="s2">]: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">numeric_features</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">numeric_features</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Categorical features processed [using </span><span class="si">{</span><span class="n">categorical_processor_name</span><span class="si">}</span><span class="s2">]: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">categorical_features</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">categorical_features</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Text features processed [using </span><span class="si">{</span><span class="n">text_processor_name</span><span class="si">}</span><span class="s2">]: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text_features</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">text_features</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Datetime features processed [using </span><span class="si">{</span><span class="n">datetime_processor_name</span><span class="si">}</span><span class="s2">]: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">datetime_features</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">datetime_features</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># define unprocessed features and output if any</span>
        <span class="n">processed_features</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">numeric_features</span> <span class="o">+</span> <span class="n">categorical_features</span> <span class="o">+</span> <span class="n">text_features</span> <span class="o">+</span> <span class="n">datetime_features</span><span class="p">)</span>
        <span class="n">all_features</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">x_cols</span><span class="p">)</span>
        <span class="n">unprocessed_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">all_features</span> <span class="o">-</span> <span class="n">processed_features</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unprocessed_features</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Unprocessed features: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">unprocessed_features</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">return</span> <span class="n">x_train_processed</span><span class="p">,</span> <span class="n">x_test_processed</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">task_type</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&lt; PREPROCESSING DATA REPORT &gt;</span><span class="se">\n</span><span class="s2">   No preprocessing was done as the user indicated the data had already been preprocessed prior to using predict_ml().</span><span class="se">\n</span><span class="s2">     [parameter: data_state = &#39;</span><span class="si">{</span><span class="n">data_state</span><span class="si">}</span><span class="s2">&#39;]</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">task_type</span>


<span class="k">def</span> <span class="nf">calculate_composite_score</span><span class="p">(</span><span class="n">scores</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">metric_weights</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Calculates a composite score based on individual metric scores and their respective weights.**</span>

<span class="sd">    This function aggregates multiple evaluation metrics into a single composite score by weighting each metric according to its importance, as defined in &#39;metric_weights&#39;. A higher weight signifies greater importance of the metric towards the composite score. This approach allows for a balanced evaluation of model performance across various aspects.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    scores : dict</span>
<span class="sd">        A dictionary where keys are metric names (str) and values are their corresponding scores (float).</span>
<span class="sd">            - Example: ``{&#39;Accuracy&#39;: 0.95, &#39;Precision&#39;: 0.90}``</span>

<span class="sd">    metric_weights : dict</span>
<span class="sd">        A dictionary where keys are metric names (str) and values are the weights (int or float) assigned to each metric.</span>
<span class="sd">            - Example: ``{&#39;Accuracy&#39;: 5, &#39;Precision&#39;: 1}``</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    float</span>
<span class="sd">        The composite score calculated as the weighted average of the provided metric scores.</span>

<span class="sd">    Raises:</span>
<span class="sd">    -------</span>
<span class="sd">    TypeError:</span>
<span class="sd">        - If &#39;scores&#39; or &#39;metric_weights&#39; is not a dictionary.</span>
<span class="sd">    ValueErrors:</span>
<span class="sd">        - If &#39;scores&#39; or &#39;metric_weights&#39; is empty.</span>
<span class="sd">        - If there are missing metric weights for any of the metrics provided in &#39;scores&#39;.</span>

<span class="sd">    Examples:</span>
<span class="sd">    ---------</span>
<span class="sd">    &gt;&gt;&gt; scores = {&#39;Accuracy&#39;: 0.95, &#39;Precision&#39;: 0.90}</span>
<span class="sd">    &gt;&gt;&gt; metric_weights = {&#39;Accuracy&#39;: 5, &#39;Precision&#39;: 1}</span>
<span class="sd">    &gt;&gt;&gt; composite_score = calculate_composite_score(scores, metric_weights)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;Composite Score: {composite_score:.2f}&quot;)</span>

<span class="sd">    Notes:</span>
<span class="sd">    ------</span>
<span class="sd">    - This function is utilized within the `model_recommendation_core()` of the `predict_ml()` pipeline, aimed at recommending the top `n` models based on a synthesized performance evaluation.</span>
<span class="sd">    - The composite score is derived by assigning weights to various scoring metrics, thereby enabling a prioritized and balanced assessment of model performance across multiple criteria.</span>
<span class="sd">    - Metrics for which lower values are traditionally better (e.g., RMSE) are transformed (either inverted or negated) prior to weight application, aligning all metrics to the &quot;higher is better&quot; principle for composite score calculation.</span>
<span class="sd">    - The calculation involves weighting each metric&#39;s score, summing these weighted scores, and normalizing the sum by the total weight, as detailed in the following formula:</span>

<span class="sd">        $$</span>
<span class="sd">        C = \frac{\sum_{m \in M} (w_m \cdot \text{adj}(s_m))}{\sum_{m \in M} w_m}</span>
<span class="sd">        $$</span>

<span class="sd">        where $\text{adj}(s_m)$ is the score adjustment function, ensuring a consistent interpretation across metrics, $w_m$ represents the weight of metric $m$, and $M$ is the set of all metrics.</span>

<span class="sd">    - The inversion or negation of scores for metrics where lower values are preferable ensures the composite score accurately reflects a model&#39;s overall efficacy, facilitating straightforward comparisons across diverse model configurations.</span>
<span class="sd">    - Review of metrics&#39; adherence to the &#39;higher is better&#39; framework indicates the systematic alignment of evaluation metrics, reinforcing the utility and interpretability of the composite scoring approach in model selection processes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Error Handling #</span>

    <span class="c1"># TypeErrors</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metric_weights</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;calculate_composite_score(): Both &#39;scores&#39; and &#39;metric_weights&#39; must be dictionaries.&quot;</span><span class="p">)</span>

    <span class="c1"># ValueErrors</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">scores</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">metric_weights</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;calculate_composite_score(): &#39;scores&#39; and &#39;metric_weights&#39; dictionaries cannot be empty.&quot;</span><span class="p">)</span>

    <span class="n">missing_metrics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">metric_weights</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">missing_metrics</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;calculate_composite_score(): Missing metric weights for: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">missing_metrics</span><span class="p">)</span><span class="si">}</span><span class="s2">. Ensure &#39;metric_weights&#39; includes all necessary metrics.&quot;</span><span class="p">)</span>

    <span class="c1"># Main Function #</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">composite_score</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">score</span> <span class="o">*</span> <span class="n">metric_weights</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">scores</span><span class="o">.</span><span class="n">items</span><span class="p">())</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">metric_weights</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;calculate_composite_score(): Error calculating composite score: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">composite_score</span>


<span class="k">def</span> <span class="nf">model_recommendation_core</span><span class="p">(</span>
        <span class="n">x_train</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
        <span class="n">y_train</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
        <span class="n">task_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">cv</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">priority_metrics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">n_top_models</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Recommends top N machine learning models based on composite scores derived from multiple evaluation metrics.**</span>

<span class="sd">    This function is part of a broader machine learning pipeline, designed to facilitate model selection by automatically evaluating a range of models against a set of performance metrics, tailored to the specific needs of the analysis.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    x_train : Union[pd.DataFrame, np.ndarray]</span>
<span class="sd">        Training feature dataset.</span>

<span class="sd">    y_train : Union[pd.Series, np.ndarray]</span>
<span class="sd">        Training target variable.</span>

<span class="sd">    task_type : str</span>
<span class="sd">        Specifies the type of machine learning task: ``&#39;classification&#39;`` or ``&#39;regression&#39;``.</span>

<span class="sd">    priority_metrics : List[str], optional, default: []</span>
<span class="sd">        List of metric names given priority in model scoring.</span>

<span class="sd">    cv: int, optional, default: 5</span>
<span class="sd">        Determines the cross-validation splitting strategy.</span>

<span class="sd">    n_top_models : int, optional, default: 3</span>
<span class="sd">        Number of top models to recommend.</span>

<span class="sd">    verbose : int, optional, verbose: 1</span>
<span class="sd">        The higher value the more output and information the user receives.</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    Dict[str, Any]</span>
<span class="sd">        Dictionary of top N recommended models, keyed by model name with model object as value.</span>

<span class="sd">    Raises:</span>
<span class="sd">    -------</span>
<span class="sd">    TypeErrors:</span>
<span class="sd">        - If &#39;x_train&#39; is not a pandas DataFrame or NumPy ndarray.</span>
<span class="sd">        - If &#39;y_train&#39; is not a pandas Series or NumPy ndarray.</span>
<span class="sd">        - If &#39;priority_metrics&#39; is not a list.</span>
<span class="sd">        - If &#39;verbose&#39; is not an integer.</span>
<span class="sd">    ValueErrors:</span>
<span class="sd">        - If &#39;task_type&#39; is not &#39;classification&#39; or &#39;regression&#39;.</span>
<span class="sd">        - If &#39;n_top_models&#39; is not an integer greater than 0.</span>
<span class="sd">        - If &#39;x_train&#39; and &#39;y_train&#39; do not have the same number of rows.</span>
<span class="sd">        - If &#39;x_train&#39; or &#39;y_train&#39; is empty.</span>
<span class="sd">        - If &#39;priority_metrics&#39; contains duplicate values or items not representing metric names as strings.</span>
<span class="sd">        - If provided metric names in &#39;priority_metrics&#39; are invalid or not supported, listing valid metric names for reference.</span>
<span class="sd">        - If provided metric names in &#39;priority_metrics&#39; are not suitable for the &#39;task_type&#39;, listing valid metrics names for reference.</span>
<span class="sd">        - If &#39;n_top_models&#39; exceeds the number of available models for the specified &#39;task_type&#39;.</span>


<span class="sd">    Notes:</span>
<span class="sd">    ------</span>
<span class="sd">    The core leverages a composite score for model evaluation, which synthesizes scores across multiple metrics, weighted by the specified priorities. This method enables a holistic and nuanced model comparison, taking into account the multidimensional aspects of model performance.</span>

<span class="sd">        - **Priority Metrics:** Assigning weights (default: 5 for prioritized metrics, 1 for others) allows users to emphasize metrics they find most relevant, affecting the composite score calculation.</span>

<span class="sd">        - **Composite Score:** Calculated as a weighted average of metric scores, normalized by the total weight. This score serves as a basis for ranking models.</span>

<span class="sd">        - **Tips and Guidance:** Optional tips provide insights on interpreting and leveraging different metrics, enhancing informed decision-making in model selection.</span>

<span class="sd">        - **Ensuring &#39;Higher is Better&#39; Across All Metrics:** For metrics where traditionally a lower score is better (e.g., RMSE), scores are transformed to align with the &#39;higher is better&#39; principle used in composite score calculation. This transformation is inherent to the scoring configurations and does not require manual adjustment.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Error handling #</span>

    <span class="c1"># TypeErrors</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;x_train&#39; must be a pandas DataFrame or NumPy ndarray.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;y_train&#39; must be a pandas Series or NumPy ndarray.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">task_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;classification&#39;</span><span class="p">,</span> <span class="s1">&#39;regression&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;task_type&#39; must be either &#39;classification&#39; or &#39;regression&#39;.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">priority_metrics</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;priority_metrics&#39; must be a list of scoring metric names.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cv</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;cv&#39; must be an integer.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n_top_models</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">n_top_models</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;n_top_models&#39; must be an integer greater than 0.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">verbose</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;verbose&#39; must be an integer value.&quot;</span><span class="p">)</span>

    <span class="c1"># ValueErrors</span>
    <span class="c1"># ensure &#39;x_train&#39; and &#39;y_train&#39; are not empty</span>
    <span class="k">if</span> <span class="n">x_train</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;x_train&#39; cannot be empty.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_train</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;y_train&#39; cannot be empty.&quot;</span><span class="p">)</span>

    <span class="c1"># validate if x_train and y_train have compatible shapes</span>
    <span class="k">if</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;x_train&#39; and &#39;y_train&#39; must have the same number of rows.&quot;</span><span class="p">)</span>

    <span class="c1"># ensure &#39;priority_metrics&#39; does not contain duplicates</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">priority_metrics</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">priority_metrics</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): &#39;priority_metrics&#39; should not contain duplicate values.&quot;</span><span class="p">)</span>

    <span class="c1"># ensure &#39;priority_metrics&#39; list items are strings</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">priority_metrics</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core(): All items in &#39;priority_metrics&#39; must be strings representing metric names.&quot;</span><span class="p">)</span>

    <span class="c1"># check if provided metrics are valid and remind users of valid options</span>
    <span class="n">valid_metrics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">scoring_classification</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">scoring_regression</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="n">invalid_metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">metric</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">priority_metrics</span> <span class="k">if</span> <span class="n">metric</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_metrics</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">invalid_metrics</span><span class="p">:</span>
        <span class="n">valid_metric_list</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">valid_metrics</span><span class="p">))</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_recommendation_core(): Invalid metric(s) in &#39;priority_metrics&#39;: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">invalid_metrics</span><span class="p">)</span><span class="si">}</span><span class="s2">.</span><span class="se">\n\n</span><span class="s2">Valid metrics are: </span><span class="si">{</span><span class="n">valid_metric_list</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="c1"># check for valid metrics in relation to task type</span>
    <span class="n">c_valid_metrics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">scoring_classification</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="n">r_valid_metrics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">scoring_regression</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span><span class="p">:</span>
        <span class="n">invalid_metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">metric</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">priority_metrics</span> <span class="k">if</span> <span class="n">metric</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">c_valid_metrics</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">invalid_metrics</span><span class="p">:</span>
            <span class="n">valid_metric_list</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">c_valid_metrics</span><span class="p">))</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_recommendation_core(): The following priority metrics are not valid for </span><span class="si">{</span><span class="n">task_type</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">invalid_metrics</span><span class="p">)</span><span class="si">}</span><span class="s2">.</span><span class="se">\n\n</span><span class="s2">Valid metrics for </span><span class="si">{</span><span class="n">task_type</span><span class="si">}</span><span class="s2"> are: </span><span class="si">{</span><span class="n">valid_metric_list</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;regression&#39;</span><span class="p">:</span>
        <span class="n">invalid_metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">metric</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">priority_metrics</span> <span class="k">if</span> <span class="n">metric</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">r_valid_metrics</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">invalid_metrics</span><span class="p">:</span>
            <span class="n">valid_metric_list</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">r_valid_metrics</span><span class="p">))</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_recommendation_core(): The following priority metrics are not valid for </span><span class="si">{</span><span class="n">task_type</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">invalid_metrics</span><span class="p">)</span><span class="si">}</span><span class="s2">.</span><span class="se">\n\n</span><span class="s2">Valid metrics for </span><span class="si">{</span><span class="n">task_type</span><span class="si">}</span><span class="s2"> are: </span><span class="si">{</span><span class="n">valid_metric_list</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># check if &#39;n_top_models&#39; exceeds the number of available models for the task</span>
    <span class="k">if</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span> <span class="ow">and</span> <span class="n">n_top_models</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">models_classification</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_recommendation_core(): &#39;n_top_models&#39; cannot exceed the number of available classification models (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">models_classification</span><span class="p">)</span><span class="si">}</span><span class="s2">).&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;regression&#39;</span> <span class="ow">and</span> <span class="n">n_top_models</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">models_regression</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_recommendation_core(): &#39;n_top_models&#39; cannot exceed the number of available regression models (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">models_regression</span><span class="p">)</span><span class="si">}</span><span class="s2">).&quot;</span><span class="p">)</span>

    <span class="c1"># Main Function #</span>
    <span class="k">if</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span><span class="p">:</span>
        <span class="n">models</span> <span class="o">=</span> <span class="n">models_classification</span>
        <span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring_classification</span>
        <span class="n">tips_scoring</span> <span class="o">=</span> <span class="n">tips_scoring_classification</span>
    <span class="k">elif</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;regression&#39;</span><span class="p">:</span>
        <span class="n">models</span> <span class="o">=</span> <span class="n">models_regression</span>
        <span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring_regression</span>
        <span class="n">tips_scoring</span> <span class="o">=</span> <span class="n">tips_scoring_regression</span>

    <span class="c1"># generate weights for priority metrics</span>
    <span class="n">metric_weights</span> <span class="o">=</span> <span class="p">{</span><span class="n">metric_name</span><span class="p">:</span> <span class="mi">5</span> <span class="k">if</span> <span class="n">metric_func</span> <span class="ow">in</span> <span class="n">priority_metrics</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">metric_func</span> <span class="ow">in</span> <span class="n">scoring</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="n">model_scores</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">composite_scores</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">)</span>

        <span class="n">average_scores</span> <span class="o">=</span> <span class="p">{</span><span class="n">metric</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;test_</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">scoring</span><span class="p">}</span>
        <span class="n">composite_score</span> <span class="o">=</span> <span class="n">calculate_composite_score</span><span class="p">(</span><span class="n">average_scores</span><span class="p">,</span> <span class="n">metric_weights</span><span class="p">)</span>

        <span class="n">model_scores</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">average_scores</span>
        <span class="n">composite_scores</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">composite_score</span>

    <span class="n">top_models</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">composite_scores</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">composite_scores</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="n">n_top_models</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&lt; MODEL RECOMMENDATIONS &gt;&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The recommendation core has prioritized the following scoring metrics while choosing the best models: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">metric_name</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">metric_name</span><span class="p">,</span><span class="w"> </span><span class="n">metric_func</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">scoring</span><span class="o">.</span><span class="n">items</span><span class="p">()</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">metric_func</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">priority_metrics</span><span class="p">])</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">priority_metrics</span> <span class="k">else</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The recommendation core has not prioritized any metrics.</span><span class="se">\n</span><span class="s2">To prioritize a metric add it&#39;s name to the &#39;priority_metrics&#39; list parameter. (e.g. priority_metrics=[&#39;explained_variance&#39;, &#39;neg_root_mean_squared_error&#39;]&quot;</span><span class="p">)</span>
        <span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Tip on </span><span class="si">{</span><span class="n">scoring_metric</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">score_tip</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">scoring_metric</span> <span class="ow">in</span> <span class="n">priority_metrics</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span> <span class="k">for</span> <span class="n">scoring_metric</span><span class="p">,</span> <span class="n">score_tip</span> <span class="ow">in</span> <span class="n">tips_scoring</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span> <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Tip on </span><span class="si">{</span><span class="n">scoring_metric</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">score_tip</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">scoring_metric</span><span class="p">,</span> <span class="n">score_tip</span> <span class="ow">in</span> <span class="n">tips_scoring</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span> <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">3</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best untuned models:&quot;</span><span class="p">)</span>
        <span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">()&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">top_models</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">top_models</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> (Composite Score: </span><span class="si">{</span><span class="n">composite_scores</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">average_score</span> <span class="ow">in</span> <span class="n">model_scores</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">average_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">best_untuned_models</span> <span class="o">=</span> <span class="p">{</span><span class="n">model</span><span class="p">:</span> <span class="n">models</span><span class="p">[</span><span class="n">model</span><span class="p">]</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">top_models</span><span class="p">}</span>
    <span class="k">return</span> <span class="n">best_untuned_models</span>


<span class="k">def</span> <span class="nf">model_tuning_core</span><span class="p">(</span>
        <span class="n">x_train</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
        <span class="n">y_train</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
        <span class="n">task_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">models</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">priority_metrics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">refit_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">priority_tuners</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_param_grids</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">cv</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">n_iter_random</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_iter_bayesian</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">42</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Conducts hyperparameter tuning on a set of models using specified tuning methods and parameter grids, and returns the best tuned models along with their scores.**</span>

<span class="sd">    This function systematically applies grid search, random search, or Bayesian optimization to explore the hyperparameter space of given models. It supports customization of the tuning process through various parameters and outputs the best found configurations.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    x_train : Union[pd.DataFrame, np.ndarray]</span>
<span class="sd">        Training feature dataset.</span>

<span class="sd">    y_train : Union[pd.Series, np.ndarray]</span>
<span class="sd">        Training target variable.</span>

<span class="sd">    task_type : str</span>
<span class="sd">        Specifies the type of machine learning task: ``&#39;classification&#39;`` or ``&#39;regression&#39;``.</span>

<span class="sd">    models : dict</span>
<span class="sd">        Dictionary with model names as keys and model instances as values.</span>

<span class="sd">    priority_metrics : List[str], optional, default: None</span>
<span class="sd">        List of metric names given priority in model scoring.</span>

<span class="sd">    refit_metric : str, optional, default: None</span>
<span class="sd">        Metric to use for refitting the models in the machine learning pipeline. If ``None``, the function will use the first member of ``priority_matrics``. If no ``priority_metrics`` are provided, the function defaults to ``&#39;Accuracy&#39;`` for classification models and ``&#39;MSE&#39;`` for regression models.</span>

<span class="sd">    priority_tuners : List[str], optional, default: None</span>
<span class="sd">        List of tuner names to use for hyperparameter tuning. Valid tuners are ``&#39;grid&#39;``, ``&#39;random&#39;``, ``&#39;bayesian&#39;``.</span>

<span class="sd">    custom_param_grids : dict, optional, default: None</span>
<span class="sd">        Custom parameter grids to use, overriding the default grids if provided. Each entry should be a model name mapped to its corresponding parameter grid.</span>

<span class="sd">    n_jobs : int, optional, default: -1</span>
<span class="sd">        Number of jobs to run in parallel. -1 means using all processors/parallel processing.</span>

<span class="sd">    cv : int, optional, default: 5</span>
<span class="sd">        Number of cross-validation folds.</span>

<span class="sd">    n_iter_random : int, optional, default: 10</span>
<span class="sd">        Number of iterations for random search.</span>

<span class="sd">    n_iter_bayesian : int, optional, default: 50</span>
<span class="sd">        Number of iterations for Bayesian optimization.</span>

<span class="sd">    verbose : int, optional, default: 1</span>
<span class="sd">        Level of verbosity. The higher the number, the more detailed the console output.</span>

<span class="sd">    random_state : int, optional, default: 42</span>
<span class="sd">        Seed used by the random number generator.</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    Dict[str, Any]</span>
<span class="sd">        A dictionary containing the best models under each provided model name as keys. Values are dictionaries with keys: &#39;best_model&#39; storing the model object of the best estimator and &#39;best_score&#39; storing the corresponding score.</span>

<span class="sd">    Raises:</span>
<span class="sd">    -------</span>
<span class="sd">    TypeErrors:</span>
<span class="sd">        - If &#39;x_train&#39; is not a pandas DataFrame or NumPy ndarray.</span>
<span class="sd">        - If &#39;y_train&#39; is not a pandas Series or NumPy ndarray.</span>
<span class="sd">        - If &#39;task_type&#39; is not a string.</span>
<span class="sd">        - If &#39;models&#39; is not a dictionary with model names as keys and model instances as values.</span>
<span class="sd">        - If &#39;priority_metrics&#39; is not None and is not a list of strings.</span>
<span class="sd">        - If &#39;priority_tuners&#39; is not None and is not a list of strings.</span>
<span class="sd">        - If &#39;custom_param_grids&#39; is not None and is not a dictionary.</span>
<span class="sd">        - If &#39;n_jobs&#39;, &#39;cv&#39;, &#39;n_iter_random&#39;, &#39;n_iter_bayesian&#39;, &#39;verbose&#39;, or &#39;random_state&#39; are not integers.</span>
<span class="sd">        - If &#39;cv&#39; is less than 1.</span>
<span class="sd">        - If &#39;n_iter_random&#39; or &#39;n_iter_bayesian&#39; is less than 1 when not None.</span>
<span class="sd">        - If &#39;refit_metric&#39; is provided as a string but is not a callable or recognized metric name.</span>
<span class="sd">    ValueErrors:</span>
<span class="sd">        - If &#39;task_type&#39; is not &#39;classification&#39; or &#39;regression&#39;.</span>
<span class="sd">        - If &#39;x_train&#39; and &#39;y_train&#39; do not have the same number of rows.</span>
<span class="sd">        - If &#39;x_train&#39; or &#39;y_train&#39; is empty (has zero elements).</span>
<span class="sd">        - If any element in &#39;priority_metrics&#39; or &#39;priority_tuners&#39; is not a string.</span>
<span class="sd">        - If &#39;priority_metrics&#39; contains duplicate values.</span>
<span class="sd">        - If &#39;priority_tuners&#39; contains unrecognized tuner names, not part of the expected tuners (&#39;grid&#39;, &#39;random&#39;, &#39;bayesian&#39;).</span>
<span class="sd">        - If the specified &#39;refit_metric&#39; is not applicable to the provided &#39;task_type&#39; (e.g., using a regression metric for classification).</span>
<span class="sd">        - If &#39;n_iter_random_adjusted&#39; or &#39;n_iter_bayesian_adjusted&#39; becomes zero due to all combinations being previously tested, implying there are no new combinations to explore.</span>
<span class="sd">        - If &#39;n_iter_random&#39; or &#39;n_iter_bayesian&#39; is set to zero or a negative number.</span>

<span class="sd">    Notes:</span>
<span class="sd">    ------</span>
<span class="sd">        - **Integration with Tuning Methods**: This function utilizes scikit-learn&#39;s `GridSearchCV` and `RandomizedSearchCV`, along with scikit-optimize&#39;s `BayesSearchCV` for hyperparameter tuning. The choice of tuning method (`grid`, `random`, or `bayesian`) depends on the entries provided in the `priority_tuners` list.</span>

<span class="sd">        - **Skipping Repeated Combinations**: For Bayesian optimization (`BayesSearchCV`), the function is designed to skip evaluations of previously tested parameter combinations. This approach aims to enhance the efficiency and performance of the tuning process by reducing redundant computations.</span>

<span class="sd">        - **Parameter Grids Importance**: The quality and range of the parameter grids significantly influence the effectiveness of the tuning process. While default parameter grids are provided for convenience, it is recommended to supply customized parameter grids via `custom_param_grids` to ensure a thorough exploration of meaningful parameter combinations.</span>

<span class="sd">        - **Handling of User Warnings**: The `BayesSearchCV` from scikit-optimize occasionally emits warnings about the evaluation of repeated parameter points. This is a known issue within the library, unresolved for several years, which pertains to its internal handling of random state and the stochastic nature of the search algorithm. Although the function attempts to mitigate this by filtering out previously tested combinations, some warnings might still appear, especially under constraints of limited parameter grids or high `n_iter_bayesian` values.</span>

<span class="sd">        - **Refit Consideration**: The refit process, which re-trains the best estimator on the full dataset using the best-found parameters, is governed by the `refit_metric`. This metric should be carefully chosen to align with the overall objective and the specifics of the task at hand (classification or regression).</span>

<span class="sd">        - **Random State Usage**: The `random_state` parameter ensures reproducibility in the results of randomized search methods and Bayesian optimization, making the tuning outputs deterministic and easier to debug or review.</span>

<span class="sd">        - **Parallel Processing Capability**: Setting `n_jobs=-1` enables the function to use all available CPU cores for parallel processing, speeding up the computation especially beneficial when dealing with large datasets or complex models.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Error Handling #</span>

    <span class="c1"># TypeErrors</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;x_train&#39; must be a pandas DataFrame or NumPy ndarray.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;y_train&#39; must be a pandas Series or NumPy ndarray.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;task_type&#39; must be a string.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">models</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;models&#39; must be a dictionary with model names as keys and model instances as values.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">priority_metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">priority_metrics</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;priority_metrics&#39; must be a list of strings.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">priority_tuners</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">priority_tuners</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;priority_tuners&#39; must be a list of strings.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">custom_param_grids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">custom_param_grids</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;custom_param_grids&#39; must be a dictionary of parameter grids.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n_jobs</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;n_jobs&#39; must be an integer.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cv</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;cv&#39; must be an integer.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_iter_random</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n_iter_random</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">n_iter_random</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;n_iter_random&#39; must be a non-negative integer.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_iter_bayesian</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n_iter_bayesian</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">n_iter_bayesian</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;n_iter_bayesian&#39; must be a non-negative integer.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">verbose</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;verbose&#39; must be an integer.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;random_state&#39; must be an integer.&quot;</span><span class="p">)</span>

    <span class="c1"># ValueErrors</span>
    <span class="k">if</span> <span class="n">task_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;classification&#39;</span><span class="p">,</span> <span class="s1">&#39;regression&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;task_type&#39; must be either &#39;classification&#39; or &#39;regression&#39;.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;x_train&#39; and &#39;y_train&#39; must have the same number of rows.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x_train</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;x_train&#39; cannot be empty.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">y_train</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;y_train&#39; cannot be empty.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">priority_metrics</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">priority_metrics</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): All elements in &#39;priority_metrics&#39; must be strings.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">priority_tuners</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tuner</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">tuner</span> <span class="ow">in</span> <span class="n">priority_tuners</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): All elements in &#39;priority_tuners&#39; must be strings representing the tuners&#39; short names.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">priority_metrics</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">priority_metrics</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">priority_metrics</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;priority_metrics&#39; contains duplicate entries.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cv</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_tuning_core(): &#39;cv&#39; must be an integer greater than 0.&quot;</span><span class="p">)</span>

    <span class="c1"># Check valid metrics based on task_type</span>
    <span class="n">valid_metrics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">scoring_classification</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="k">if</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span> <span class="k">else</span> <span class="nb">set</span><span class="p">(</span><span class="n">scoring_regression</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="n">invalid_metrics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">priority_metrics</span><span class="p">)</span> <span class="o">-</span> <span class="n">valid_metrics</span> <span class="k">if</span> <span class="n">priority_metrics</span> <span class="k">else</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">invalid_metrics</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_tuning_core(): The following metrics are not valid for </span><span class="si">{</span><span class="n">task_type</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">invalid_metrics</span><span class="p">)</span><span class="si">}</span><span class="s2">. Valid metrics are: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">valid_metrics</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="c1"># Checking valid tuners</span>
    <span class="n">valid_tuners</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;grid&#39;</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="s1">&#39;bayesian&#39;</span><span class="p">}</span>  <span class="c1"># Example of possible tuner names</span>
    <span class="n">invalid_tuners</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">priority_tuners</span><span class="p">)</span> <span class="o">-</span> <span class="n">valid_tuners</span> <span class="k">if</span> <span class="n">priority_tuners</span> <span class="k">else</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">invalid_tuners</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_tuning_core(): The following tuners are not recognized: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">invalid_tuners</span><span class="p">)</span><span class="si">}</span><span class="s2">. Valid tuners are: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">valid_tuners</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="c1"># Check refit_metric validity</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">refit_metric</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">refit_metric</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_metrics</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_tuning_core(): &#39;refit_metric&#39; </span><span class="si">{</span><span class="n">refit_metric</span><span class="si">}</span><span class="s2"> is not a valid metric. Choose from </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">valid_metrics</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="c1"># Main Functionality #</span>
    <span class="c1"># initialize tracking for tested parameter combinations</span>
    <span class="n">tested_combinations</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">set</span><span class="p">)</span>

    <span class="c1"># define default iterations if none provided</span>
    <span class="n">n_iter_random</span> <span class="o">=</span> <span class="n">n_iter_random</span> <span class="ow">or</span> <span class="mi">10</span>
    <span class="n">n_iter_bayesian</span> <span class="o">=</span> <span class="n">n_iter_bayesian</span> <span class="ow">or</span> <span class="mi">50</span>

    <span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring_classification</span> <span class="k">if</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span> <span class="k">else</span> <span class="n">scoring_regression</span>
    <span class="n">priority_scoring</span> <span class="o">=</span> <span class="p">{</span><span class="n">metric_name</span><span class="p">:</span> <span class="n">metric_func</span> <span class="k">for</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">metric_func</span> <span class="ow">in</span> <span class="n">scoring</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">metric_func</span> <span class="ow">in</span> <span class="n">priority_metrics</span><span class="p">}</span> <span class="k">if</span> <span class="n">priority_metrics</span> <span class="k">else</span> <span class="n">scoring</span>

    <span class="k">if</span> <span class="n">refit_metric</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">priority_metrics</span><span class="p">:</span>
            <span class="n">first_priority_metric_name</span> <span class="o">=</span> <span class="p">[</span><span class="n">metric_name</span> <span class="k">for</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">metric_func</span> <span class="ow">in</span> <span class="n">scoring</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">metric_func</span> <span class="o">==</span> <span class="n">priority_metrics</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="n">refit_metric</span> <span class="o">=</span> <span class="n">first_priority_metric_name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">refit_metric</span> <span class="o">=</span> <span class="s1">&#39;Accuracy&#39;</span> <span class="k">if</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span> <span class="k">else</span> <span class="s1">&#39;MSE&#39;</span>

    <span class="n">final_param_grids</span> <span class="o">=</span> <span class="n">default_param_grids_classification</span> <span class="k">if</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span> <span class="k">else</span> <span class="n">default_param_grids_regression</span>
    <span class="k">if</span> <span class="n">custom_param_grids</span><span class="p">:</span>
        <span class="n">final_param_grids</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">custom_param_grids</span><span class="p">)</span>

    <span class="n">model_tuners</span> <span class="o">=</span> <span class="p">{</span><span class="n">tuner_name</span><span class="p">:</span> <span class="n">tuners</span><span class="p">[</span><span class="n">tuner_name</span><span class="p">]</span> <span class="k">for</span> <span class="n">tuner_name</span> <span class="ow">in</span> <span class="n">priority_tuners</span><span class="p">}</span> <span class="k">if</span> <span class="n">priority_tuners</span> <span class="k">else</span> <span class="n">tuners</span>

    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&lt; TUNING INITIATED &gt;&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting model tuning for </span><span class="si">{</span><span class="n">task_type</span><span class="si">}</span><span class="s2"> modelling.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">priority_tuners</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Priority tuner(s): </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">tuner_names</span><span class="p">[</span><span class="n">tuner_short_name</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">tuner_short_name</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_tuners</span><span class="o">.</span><span class="n">keys</span><span class="p">()])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Priority tuners: None (using default: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">tuner_names</span><span class="p">[</span><span class="n">tuner_short_name</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">tuner_short_name</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_tuners</span><span class="o">.</span><span class="n">keys</span><span class="p">()])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">priority_metrics</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Priority metrics: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">priority_metrics</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Priority metrics: None (using default scoring criteria)&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Refit metric: </span><span class="si">{</span><span class="n">refit_metric</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">custom_param_grids</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Parameter grid: Custom (defined by user)&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Parameter grid: Default&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_jobs</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Parallel processing: ON&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Parallel processing: OFF (n_jobs = </span><span class="si">{</span><span class="n">n_jobs</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Random state: </span><span class="si">{</span><span class="n">random_state</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">tuned_models</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">model_object</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">param_grid</span> <span class="o">=</span> <span class="n">final_param_grids</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="p">{})</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">param_grid</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Notice: Skipping tuning for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> as no parameter grid is provided.&quot;</span><span class="p">)</span>
            <span class="k">continue</span>

        <span class="n">total_combinations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>
        <span class="n">tested_fraction</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tested_combinations</span><span class="p">[</span><span class="n">model_name</span><span class="p">])</span> <span class="o">/</span> <span class="n">total_combinations</span> <span class="k">if</span> <span class="n">total_combinations</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="n">n_iter_random_adjusted</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_iter_random</span><span class="p">,</span> <span class="nb">int</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tested_fraction</span><span class="p">)</span> <span class="o">*</span> <span class="n">total_combinations</span><span class="p">))</span>
        <span class="n">n_iter_bayesian_adjusted</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_iter_bayesian</span><span class="p">,</span> <span class="nb">int</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tested_fraction</span><span class="p">)</span> <span class="o">*</span> <span class="n">total_combinations</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">tuner_short_name</span><span class="p">,</span> <span class="n">tuner_class</span> <span class="ow">in</span> <span class="n">model_tuners</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>

            <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Tuning model: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Total possible param grid combinations: </span><span class="si">{</span><span class="n">total_combinations</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>

            <span class="k">if</span> <span class="n">tuner_short_name</span> <span class="o">==</span> <span class="s1">&#39;grid&#39;</span><span class="p">:</span>
                <span class="n">tuner</span> <span class="o">=</span> <span class="n">tuner_class</span><span class="p">(</span>
                    <span class="n">estimator</span><span class="o">=</span><span class="n">model_object</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">priority_scoring</span><span class="p">,</span>
                    <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="n">refit_metric</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; Running </span><span class="si">{</span><span class="n">tuner_names</span><span class="p">[</span><span class="n">tuner_short_name</span><span class="p">]</span><span class="si">}</span><span class="s2"> for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">tuner_short_name</span> <span class="o">==</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span>
                <span class="n">tuner</span> <span class="o">=</span> <span class="n">tuner_class</span><span class="p">(</span>
                    <span class="n">estimator</span><span class="o">=</span><span class="n">model_object</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter_random_adjusted</span><span class="p">,</span>
                    <span class="n">scoring</span><span class="o">=</span><span class="n">priority_scoring</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="n">refit_metric</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; Running </span><span class="si">{</span><span class="n">tuner_names</span><span class="p">[</span><span class="n">tuner_short_name</span><span class="p">]</span><span class="si">}</span><span class="s2"> for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">n_iter_random</span> <span class="o">&gt;</span> <span class="n">n_iter_random_adjusted</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Notice: n_iter_random=</span><span class="si">{</span><span class="n">n_iter_random</span><span class="si">}</span><span class="s2"> is reduced to max possible iterations </span><span class="si">{</span><span class="n">n_iter_random_adjusted</span><span class="si">}</span><span class="s2"> for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">tuner_short_name</span> <span class="o">==</span> <span class="s1">&#39;bayesian&#39;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">n_iter_bayesian_adjusted</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">tuner</span> <span class="o">=</span> <span class="n">tuner_class</span><span class="p">(</span>
                        <span class="n">estimator</span><span class="o">=</span><span class="n">model_object</span><span class="p">,</span> <span class="n">search_spaces</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter_bayesian_adjusted</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">priority_scoring</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="n">refit_metric</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; Running </span><span class="si">{</span><span class="n">tuner_names</span><span class="p">[</span><span class="n">tuner_short_name</span><span class="p">]</span><span class="si">}</span><span class="s2"> for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">n_iter_bayesian</span> <span class="o">&gt;</span> <span class="n">n_iter_bayesian_adjusted</span><span class="p">:</span>
                            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Notice: n_iter_bayesian=</span><span class="si">{</span><span class="n">n_iter_bayesian</span><span class="si">}</span><span class="s2"> is reduced to max possible iterations </span><span class="si">{</span><span class="n">n_iter_bayesian_adjusted</span><span class="si">}</span><span class="s2"> for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Notice: All parameter combinations for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> have been tested. Skipping Bayesian optimization.&quot;</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
            <span class="n">best_model</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">best_estimator_</span>
            <span class="n">best_score</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">best_score_</span>

            <span class="k">if</span> <span class="n">model_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tuned_models</span> <span class="ow">or</span> <span class="n">best_score</span> <span class="o">&gt;</span> <span class="n">tuned_models</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="s1">&#39;best_score&#39;</span><span class="p">]:</span>
                <span class="n">tuned_models</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;best_model&#39;</span><span class="p">:</span> <span class="n">best_model</span><span class="p">,</span> <span class="s1">&#39;best_score&#39;</span><span class="p">:</span> <span class="n">best_score</span><span class="p">}</span>
                <span class="n">tested_combinations</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">tuner</span><span class="o">.</span><span class="n">best_params_</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&lt; TUNING COMPLETED &gt;&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; Tuning summary:&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">details</span> <span class="ow">in</span> <span class="n">tuned_models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">params_str</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">details</span><span class="p">[</span><span class="s1">&#39;best_model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Best tuned version of </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">params_str</span><span class="si">}</span><span class="s2">) with score: </span><span class="si">{</span><span class="n">details</span><span class="p">[</span><span class="s1">&#39;best_score&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tuned_models</span>


<span class="k">def</span> <span class="nf">model_recommendation_core_inference</span><span class="p">(</span>
        <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
        <span class="n">formula</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">priority_models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_top_models</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Recommends top statistical models for inference based on user-specified preferences and formula.**</span>

<span class="sd">    This function evaluates various statistical models from statsmodels, each suitable for either regression or classification tasks determined dynamically by the nature of the target variable.</span>


<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    df : pd.DataFrame</span>
<span class="sd">        DataFrame containing the data to fit the models.</span>

<span class="sd">    formula : str</span>
<span class="sd">        A patsy formula specifying the model. The independent variable is on the left of &#39;~&#39;, while the dependent variables are on the right.</span>

<span class="sd">    priority_models : List[str], optional, default: None</span>
<span class="sd">        A list of model names to restrict the evaluation to specific models, otherwise all applicable models are evaluated.</span>

<span class="sd">    n_top_models : int, optional, default: 3</span>
<span class="sd">        Number of top-performing models to return based on sorted metrics.</span>

<span class="sd">    model_kwargs : dict, optional, default: None</span>
<span class="sd">        Dictionary mapping model names to dictionaries of additional keyword arguments to pass to the model constructors. This can be used to pass additional parameters required by specific models.</span>

<span class="sd">    verbose : int, optional, default: 1</span>
<span class="sd">        The verbosity level: 0 means silent, 1 outputs summary results, 2 includes detailed model summaries.</span>


<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    Dict[str, Any]</span>
<span class="sd">        A dictionary with model names as keys and dictionaries as values. Each dictionary contains the &#39;model&#39; object, &#39;metrics&#39; dictionary with performance metrics, and potentially &#39;summary&#39; if verbose &gt; 1.</span>


<span class="sd">    Raises:</span>
<span class="sd">    -------</span>
<span class="sd">    TypeErrors:</span>
<span class="sd">        - If &#39;df&#39; is not a pandas DataFrame, ensuring that the input data structure is correct for model fitting.</span>
<span class="sd">        - If &#39;formula&#39; is not a string, verifying that the model formula is correctly specified as a string.</span>
<span class="sd">        - If &#39;priority_models&#39; is provided and is not a list of strings, ensuring the user specifies a proper list of model names.</span>
<span class="sd">        - If &#39;model_kwargs&#39; is provided and is not a dictionary, ensuring the correct format for passing additional keyword arguments to model constructors.</span>
<span class="sd">        - If &#39;verbose&#39; is not an integer, verifying that the verbosity level is specified as an integer.</span>
<span class="sd">    ValueErrors:</span>
<span class="sd">        - If the input DataFrame is empty.</span>
<span class="sd">        - If &#39;formula&#39; does not contain exactly one &#39;~&#39;, which is necessary to separate the dependent and independent variables in the model specification.</span>
<span class="sd">        - If the specified target variable from &#39;formula&#39; is not found in the DataFrame, ensuring the formula correctly references a column in the DataFrame.</span>
<span class="sd">        - If any variables specified in the &#39;formula&#39; for independent variables are not found in the DataFrame, checking for the presence of all required variables in the DataFrame.</span>
<span class="sd">        - If &#39;n_top_models&#39; is not a positive integer, ensuring that the number of models to return is specified correctly.</span>


<span class="sd">    Examples:</span>
<span class="sd">    ---------</span>
<span class="sd">    &gt;&gt;&gt; import datasafari</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; import pandas as pd</span>
<span class="sd">    &gt;&gt;&gt; df = pd.DataFrame({</span>
<span class="sd">    ...     &#39;Age&#39;: np.random.randint(18, 70, size=100),</span>
<span class="sd">    ...     &#39;Salary&#39;: np.random.normal(50000, 15000, size=100),</span>
<span class="sd">    ...     &#39;Experience&#39;: np.random.randint(1, 30, size=100)</span>
<span class="sd">    ... })</span>
<span class="sd">    &gt;&gt;&gt; formula = &#39;Salary ~ Age + Experience&#39;</span>
<span class="sd">    &gt;&gt;&gt; best_inference_models = model_recommendation_core_inference(</span>
<span class="sd">    ...     df,</span>
<span class="sd">    ...     formula,</span>
<span class="sd">    ...     verbose=2</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; # Accessing the best model&#39;s object</span>
<span class="sd">    &gt;&gt;&gt; best_model_name = list(best_inference_models.keys())[0]</span>
<span class="sd">    &gt;&gt;&gt; best_model = best_inference_models[best_model_name][&#39;model&#39;]</span>
<span class="sd">    &gt;&gt;&gt; # Viewing the summary of the best model</span>
<span class="sd">    &gt;&gt;&gt; print(best_model.summary())</span>
<span class="sd">    &gt;&gt;&gt; # Extracting AIC of the best model</span>
<span class="sd">    &gt;&gt;&gt; best_model_aic = best_inference_models[best_model_name][&#39;metrics&#39;][&#39;AIC&#39;]</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;The best model according to AIC is {best_model_name} with an AIC of {best_model_aic:.2f}&quot;)</span>

<span class="sd">    Notes:</span>
<span class="sd">    -----</span>
<span class="sd">    - **Dynamic Model Evaluation**: Depending on the datatype of the target variable specified in the formula, the function dynamically decides whether to treat the problem as a regression or classification task, using appropriate metrics and models for each.</span>

<span class="sd">    - **Handling Model Specific Requirements**: This function allows passing custom arguments to model constructors to handle models that require specific parameters via `model_kwargs`.</span>

<span class="sd">    - **Metric Adjustments**: For metrics where a lower value is better (e.g., AIC, BIC), these are adjusted to be compared directly alongside higher-is-better metrics like R-squared, by negating their values during sorting.</span>

<span class="sd">    - **Verbose Output**: The function provides different levels of output detail which can help in diagnosing model fit or understanding model performance.</span>

<span class="sd">    - **Error Handling**: The function will report and skip models that encounter errors during fitting, allowing for robust execution even if some models are not applicable to the provided data or formula.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Error handling #</span>
    <span class="c1"># TypeErrors</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core_inference(): &#39;df&#39; must be a pandas DataFrame.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core_inference(): &#39;formula&#39; must be a string.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">priority_models</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">priority_models</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core_inference(): &#39;priority_models&#39; must be a list of strings.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n_top_models</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">n_top_models</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core_inference(): &#39;n_top_models&#39; must be an integer greater than 0.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core_inference(): &#39;model_kwargs&#39; must be a dictionary.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">verbose</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core_inference(): &#39;verbose&#39; must be an integer.&quot;</span><span class="p">)</span>

    <span class="c1"># ValueErrors</span>
    <span class="c1"># Ensure non-empty DataFrame</span>
    <span class="k">if</span> <span class="n">df</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core_inference(): The input DataFrame is empty.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">formula</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;~&#39;</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;model_recommendation_core_inference(): &#39;formula&#39; must include exactly one &#39;~&#39; to separate dependent and independent variables.&quot;</span><span class="p">)</span>

    <span class="n">y_col</span> <span class="o">=</span> <span class="n">formula</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;~&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">y_col</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_recommendation_core_inference(): Specified target variable &#39;</span><span class="si">{</span><span class="n">y_col</span><span class="si">}</span><span class="s2">&#39; is not in DataFrame.&quot;</span><span class="p">)</span>

    <span class="c1"># Check for presence of all variables specified in the formula in the DataFrame</span>
    <span class="n">independent_vars</span> <span class="o">=</span> <span class="n">formula</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;~&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">missing_vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">var</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">independent_vars</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">var</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">missing_vars</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;model_recommendation_core_inference(): The following independent variables are not in DataFrame: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">missing_vars</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="c1"># Main Functionality #</span>
    <span class="c1"># define task type based on the target variable data type</span>
    <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">evaluate_dtype</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="p">[</span><span class="n">y_col</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="s1">&#39;dict&#39;</span><span class="p">)[</span><span class="n">y_col</span><span class="p">]</span>
    <span class="n">task_type</span> <span class="o">=</span> <span class="s1">&#39;regression&#39;</span> <span class="k">if</span> <span class="n">y_dtype</span> <span class="o">==</span> <span class="s1">&#39;numerical&#39;</span> <span class="k">else</span> <span class="s1">&#39;classification&#39;</span>

    <span class="c1"># define models to choose from based on task type</span>
    <span class="n">models</span> <span class="o">=</span> <span class="n">models_classification_inference</span> <span class="k">if</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span> <span class="k">else</span> <span class="n">models_regression_inference</span>
    <span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring_classification_inference</span> <span class="k">if</span> <span class="n">task_type</span> <span class="o">==</span> <span class="s1">&#39;classification&#39;</span> <span class="k">else</span> <span class="n">scoring_regression_inference</span>

    <span class="c1"># consider priority models if any</span>
    <span class="n">models</span> <span class="o">=</span> <span class="p">{</span><span class="n">model_name</span><span class="p">:</span> <span class="n">model_func</span> <span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">model_func</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">priority_models</span> <span class="ow">and</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">priority_models</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">priority_models</span><span class="p">}</span>

    <span class="n">model_results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model_func</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># use provided kwargs if applicable for the model</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">{})</span> <span class="k">if</span> <span class="n">model_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model_func</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="n">model_func</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">scoring</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">metrics</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
                <span class="k">except</span> <span class="p">(</span><span class="ne">NotImplementedError</span><span class="p">,</span> <span class="ne">AttributeError</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> is not supported by the model </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">metrics</span><span class="p">:</span>
                <span class="n">model_results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
                    <span class="s1">&#39;metrics&#39;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">,</span>
                    <span class="s1">&#39;adjusted_metrics&#39;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="n">metric</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="n">value</span> <span class="k">if</span> <span class="n">metric</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;AIC&#39;</span><span class="p">,</span> <span class="s1">&#39;BIC&#39;</span><span class="p">]</span> <span class="k">else</span> <span class="n">value</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="p">}</span>
                <span class="p">}</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Error fitting model </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">continue</span>

    <span class="c1"># sorting models based on adjusted metrics</span>
    <span class="n">sorted_models</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
        <span class="p">[(</span><span class="n">name</span><span class="p">,</span> <span class="n">details</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">details</span> <span class="ow">in</span> <span class="n">model_results</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">isna</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">details</span><span class="p">[</span><span class="s1">&#39;metrics&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())],</span>
        <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;adjusted_metrics&#39;</span><span class="p">][</span><span class="n">metric</span><span class="p">]</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">scoring</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;adjusted_metrics&#39;</span><span class="p">]),</span>
        <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)[:</span><span class="n">n_top_models</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&lt; MODEL RECOMMENDATIONS &gt;&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Tip: Use verbose = 2 to see model summaries, or access the models directly in the returned dictionary object.&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">verbose</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">details</span> <span class="ow">in</span> <span class="n">sorted_models</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">  Model: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">details</span><span class="p">[</span><span class="s1">&#39;metrics&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="s1">&#39;model&#39;</span> <span class="ow">in</span> <span class="n">details</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">details</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

    <span class="k">return</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">details</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">details</span> <span class="ow">in</span> <span class="n">sorted_models</span><span class="p">}</span>


<span class="c1"># Main Function (front-end) #</span>
<div class="viewcode-block" id="predict_ml">
<a class="viewcode-back" href="../../../datasafari.predictor.predict_ml.html#datasafari.predictor.predict_ml">[docs]</a>
<span class="k">def</span> <span class="nf">predict_ml</span><span class="p">(</span>
        <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
        <span class="n">x_cols</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">y_col</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">formula</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">data_state</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;unprocessed&#39;</span><span class="p">,</span>
        <span class="n">n_top_models</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">test_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">cv</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span>
        <span class="n">priority_metrics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">refit_metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">priority_tuners</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">custom_param_grids</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">n_iter_random</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_iter_bayesian</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">priority_models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">numeric_imputer</span><span class="p">:</span> <span class="n">TransformerMixin</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;median&#39;</span><span class="p">),</span>
        <span class="n">numeric_scaler</span><span class="p">:</span> <span class="n">TransformerMixin</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">(),</span>
        <span class="n">categorical_imputer</span><span class="p">:</span> <span class="n">TransformerMixin</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="s1">&#39;missing&#39;</span><span class="p">),</span>
        <span class="n">categorical_encoder</span><span class="p">:</span> <span class="n">TransformerMixin</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">),</span>
        <span class="n">text_vectorizer</span><span class="p">:</span> <span class="n">TransformerMixin</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(),</span>
        <span class="n">datetime_transformer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">],</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    **Streamline the entire process of data preprocessing, model selection, and tuning, delivering optimal model recommendations based on the data provided.**</span>

<span class="sd">    Depending on the inputs, this function can either perform statistical inference or predictive model selection using machine learning.</span>
<span class="sd">        - **Machine Learning Pipeline**: Focuses on predictive model selection and hyperparameter tuning using scikit-learn. It includes preprocessing (optional), model recommendation based on specified metrics, and tuning using grid search, random search, or Bayesian optimization.</span>
<span class="sd">        - **Inference Pipeline**: Utilizes statsmodels for detailed statistical analysis and model fitting based on a specified formula. This pipeline is tailored for users seeking statistical inference, providing metrics such as AIC, BIC, and R-squared. This pipeline assumes the data to have been preprocessed appropriately beforehand.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    df : pd.DataFrame</span>
<span class="sd">        The DataFrame containing the dataset to be analyzed.</span>

<span class="sd">    x_cols : List[str], optional, default: None</span>
<span class="sd">        List of column names to be used as features for machine learning model recommendation.</span>

<span class="sd">    y_col : str, optional, default: None</span>
<span class="sd">        Column name to be used as the target for machine learning model recommendation.</span>

<span class="sd">    formula : str, optional, default: None</span>
<span class="sd">        A Patsy formula for specifying the model in the case of statistical inference.</span>

<span class="sd">    data_state : str, optional, default: &#39;unprocessed&#39;</span>
<span class="sd">        Specifies the initial state of the data (``&#39;unprocessed&#39;`` or ``&#39;preprocessed&#39;``).</span>

<span class="sd">        - ``&#39;unprocessed&#39;`` will trigger the customizable preprocessing procedure.</span>
<span class="sd">        - ``&#39;preprocessed&#39;`` will omit the preprocessing procedure. *Only suitable for preprocessed data!*</span>

<span class="sd">    n_top_models : int, optional, default: 3</span>
<span class="sd">        Number of top models to recommend from the evaluation.</span>

<span class="sd">    test_size : float, optional, default: 0.2</span>
<span class="sd">        Proportion of the dataset to be used as the test set.</span>

<span class="sd">    cv : int, optional, default: 5</span>
<span class="sd">        Number of cross-validation folds.</span>

<span class="sd">    random_state : int, optional, default: 42</span>
<span class="sd">        Controls the shuffling applied to the data before applying the split.</span>

<span class="sd">    priority_metrics : List[str], optional, default: []</span>
<span class="sd">        Metrics to prioritize in model evaluation in the machine learning pipeline. *Note: The list members must be in the correct format as specified below.*</span>

<span class="sd">            **Available Metrics:**</span>

<span class="sd">            - **Regression:** ``&#39;explained_variance&#39;``, ``&#39;neg_mean_absolute_error&#39;``, ``&#39;neg_mean_squared_error&#39;``, ``&#39;neg_root_mean_squared_error&#39;``, ``&#39;neg_mean_squared_log_error&#39;``, ``&#39;neg_median_absolute_error&#39;``, ``&#39;r2&#39;``, ``&#39;neg_mean_poisson_deviance&#39;``, ``&#39;neg_mean_gamma_deviance&#39;``, ``&#39;neg_mean_absolute_percentage_error&#39;``</span>
<span class="sd">            - **Classification:** ``&#39;accuracy&#39;``, ``&#39;balanced_accuracy&#39;``, ``&#39;average_precision&#39;``, ``&#39;neg_brier_score&#39;``, ``&#39;f1_micro&#39;``, ``&#39;f1_macro&#39;``, ``&#39;f1_weighted&#39;``, ``&#39;neg_log_loss&#39;``, ``&#39;precision_micro&#39;``, ``&#39;precision_macro&#39;``, ``&#39;precision_weighted&#39;``, ``&#39;recall_micro&#39;``, ``&#39;recall_macro&#39;``, ``&#39;recall_weighted&#39;``, ``&#39;jaccard_micro&#39;``, ``&#39;jaccard_macro&#39;``, ``&#39;jaccard_weighted&#39;``, ``&#39;roc_auc_ovr&#39;``, ``&#39;roc_auc_ovo&#39;``</span>

<span class="sd">    refit_metric : str, optional, default: None</span>
<span class="sd">        Metric to use for refitting the models in the machine learning pipeline. *Note: The string must be in the correct format as specified below.*</span>

<span class="sd">        - If ``None``, the function will use the first member of ``priority_matrics``.</span>
<span class="sd">        - If ``None`` and no ``priority_metrics`` are provided, the function defaults to ``&#39;Accuracy&#39;`` for classification models and ``&#39;MSE&#39;`` for regression models.</span>

<span class="sd">            **Available Refit Metrics:**</span>

<span class="sd">            - **Regression:** ``&#39;EV&#39;``, ``&#39;MAE&#39;``, ``&#39;MSE&#39;``, ``&#39;RMSE&#39;``, ``&#39;MSLE&#39;``, ``&#39;MedAE&#39;``, ``&#39;R2&#39;``, ``&#39;MPD&#39;``, ``&#39;MGD&#39;``, ``&#39;MAPE&#39;``</span>
<span class="sd">            - **Classification:** ``&#39;Accuracy&#39;``, ``&#39;Balanced Accuracy&#39;``, ``&#39;Average Precision&#39;``, ``&#39;Neg Brier Score&#39;``, ``&#39;F1 (Micro)&#39;``, ``&#39;F1 (Macro)&#39;``, ``&#39;F1 (Weighted)&#39;``, ``&#39;Neg Log Loss&#39;``, ``&#39;Precision (Micro)&#39;``, ``&#39;Precision (Macro)&#39;``, ``&#39;Precision (Weighted)&#39;``, ``&#39;Recall (Micro)&#39;``, ``&#39;Recall (Macro)&#39;``, ``&#39;Recall (Weighted)&#39;``, ``&#39;Jaccard (Micro)&#39;``, ``&#39;Jaccard (Macro)&#39;``, ``&#39;Jaccard (Weighted)&#39;``, ``&#39;ROC AUC (OVR)&#39;``, ``&#39;ROC AUC (OVO)&#39;``</span>

<span class="sd">    priority_tuners : List[str], optional, default: None</span>
<span class="sd">        Tuners to use for hyperparameter tuning in the machine learning pipeline. *Note: The list members must be in the correct format as specified below.*</span>

<span class="sd">            **Available Tuners:** ``&#39;grid&#39;``, ``&#39;random&#39;``, ``&#39;bayesian&#39;``</span>

<span class="sd">    custom_param_grids : dict, optional, default: None</span>
<span class="sd">        Custom parameter grids for tuning in the machine learning pipeline. *Note: Template dictionaries are provided at the end of this page.*</span>

<span class="sd">    n_jobs : int, optional, default: -1</span>
<span class="sd">        Number of jobs to run in parallel. -1 means using all processors/parallel processing.</span>

<span class="sd">    n_iter_random : int, optional, default: None</span>
<span class="sd">        Number of iterations for random search tuning in the machine learning pipeline.</span>

<span class="sd">    n_iter_bayesian : int, optional, default: None</span>
<span class="sd">        Number of iterations for Bayesian optimization in the machine learning pipeline.</span>

<span class="sd">    priority_models : List[str], optional, default: None</span>
<span class="sd">        Specific models to evaluate in the inference pipeline. *Note: The list members must be in the correct format as specified below.*</span>

<span class="sd">        - If ``None`` the function will assess all appropriate models.</span>

<span class="sd">            **Available Inferential Models:**</span>

<span class="sd">            - **Regression:** ``&#39;OLS&#39;``, ``&#39;WLS&#39;``, ``&#39;GLS&#39;``, ``&#39;RLM&#39;``, ``&#39;QuantReg&#39;``, ``&#39;GLSAR&#39;``, ``&#39;MixedLM&#39;``, ``&#39;PHReg&#39;``</span>
<span class="sd">            - **Classification:** ``&#39;Logit&#39;``, ``&#39;Probit&#39;``, ``&#39;MNLogit&#39;``, ``&#39;Poisson&#39;``, ``&#39;NegativeBinomial&#39;``, ``&#39;GEE&#39;``, ``&#39;NominalGEE&#39;``, ``&#39;OrdinalGEE&#39;``</span>

<span class="sd">    model_kwargs : dict, optional, default: None</span>
<span class="sd">        Keyword arguments to pass to model constructors in the inference pipeline.</span>

<span class="sd">    verbose : int, optional, default: 1</span>
<span class="sd">        Level of verbosity in output.</span>

<span class="sd">    numeric_imputer : TransformerMixin, optional, default: SimpleImputer(strategy=&#39;median&#39;)</span>
<span class="sd">        Imputer for handling missing values in numerical data, if ``data_state=&#39;unprocessed&#39;``.</span>

<span class="sd">            Any imputer from ``scikit.impute`` can be used instead of the default.</span>

<span class="sd">    numeric_scaler : TransformerMixin, optional, default: StandardScaler()</span>
<span class="sd">        Scaler for numerical data, if ``data_state=&#39;unprocessed&#39;``.</span>

<span class="sd">           Any scaler from ``scikit.preprocessing`` can be used instead of the default.</span>

<span class="sd">    categorical_imputer : TransformerMixin, optional, default: SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;missing&#39;)</span>
<span class="sd">        Imputer for handling missing values in categorical data, if ``data_state=&#39;unprocessed&#39;``.</span>

<span class="sd">           Any imputer from ``scikit.preprocessing`` can be used instead of the default.</span>

<span class="sd">    categorical_encoder : TransformerMixin, optional, default: OneHotEncoder(handle_unknown=&#39;ignore&#39;)</span>
<span class="sd">        Encoder for categorical data, if ``data_state=&#39;unprocessed&#39;``.</span>

<span class="sd">            Any enoder from ``scikit.preprocessing`` can be used instead of the default.</span>

<span class="sd">    text_vectorizer : TransformerMixin, optional, default: CountVectorizer()</span>
<span class="sd">        Vectorizer for text data, if ``data_state=&#39;unprocessed&#39;``.</span>

<span class="sd">            Any vectorizer from ``sklearn.feature_extraction.text`` can be used instead of the default.</span>

<span class="sd">    datetime_transformer : callable, optional, default: None</span>
<span class="sd">        Transformer for datetime data, if ``data_state=&#39;unprocessed&#39;``.</span>

<span class="sd">            *Note: This parameter defaults to a custom datetime transformer, which extracts year, month, and day as separate features. This is an experimental feature and it is not recommended to use other solutions.*</span>


<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    Dict[str, Any]</span>
<span class="sd">        Depending on the operation mode, the dictionary contains either:</span>
<span class="sd">            - top machine learning models and their evaluation metrics,</span>
<span class="sd">            - top statistical models along with their fit statistics.</span>


<span class="sd">    Examples:</span>
<span class="sd">    ---------</span>
<span class="sd">    **Import necessary libraries and generate a DataFrame for examples:**</span>

<span class="sd">    &gt;&gt;&gt; import datasafari</span>
<span class="sd">    &gt;&gt;&gt; import pandas as pd</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; df = pd.DataFrame({</span>
<span class="sd">    ...     &#39;Age&#39;: np.random.randint(18, 35, size=100),</span>
<span class="sd">    ...     &#39;Salary&#39;: np.random.normal(50000, 12000, size=100),</span>
<span class="sd">    ...     &#39;Department&#39;: np.random.choice([&#39;HR&#39;, &#39;Tech&#39;, &#39;Marketing&#39;], size=100),</span>
<span class="sd">    ...     &#39;Review&#39;: [&#39;Good review&#39;]*50 + [&#39;Bad review&#39;]*50,</span>
<span class="sd">    ...     &#39;Employment Date&#39;: pd.date_range(start=&#39;2010-01-01&#39;, periods=100, freq=&#39;M&#39;)</span>
<span class="sd">    ... })</span>

<span class="sd">    Machine Learning Pipeline Examples</span>
<span class="sd">    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>

<span class="sd">    **Simple Machine Learning Pipeline:**</span>

<span class="sd">    &gt;&gt;&gt; x_cols = [&#39;Age&#39;, &#39;Salary&#39;, &#39;Department&#39;, &#39;Review&#39;, &#39;Employment Date&#39;]</span>
<span class="sd">    &gt;&gt;&gt; y_col = &#39;Salary&#39;</span>
<span class="sd">    &gt;&gt;&gt; ml_models = predict_ml(df, x_cols=x_cols, y_col=y_col, verbose=2)</span>

<span class="sd">    **Utilizing Priority Metrics and Refit Metric in Machine Learning Pipeline:**</span>

<span class="sd">    &gt;&gt;&gt; priority_metrics = [&#39;neg_mean_squared_error&#39;, &#39;r2&#39;]</span>
<span class="sd">    &gt;&gt;&gt; ml_models_priority_metrics = predict_ml(</span>
<span class="sd">    ...     df,</span>
<span class="sd">    ...     x_cols=x_cols,</span>
<span class="sd">    ...     y_col=y_col,</span>
<span class="sd">    ...     priority_metrics=priority_metrics,</span>
<span class="sd">    ...     refit_metric=&#39;r2&#39;,</span>
<span class="sd">    ...     verbose=2</span>
<span class="sd">    ... )</span>

<span class="sd">    **Integrating Priority Tuners with Custom Parameter Grids:**</span>

<span class="sd">    &gt;&gt;&gt; custom_grids = {</span>
<span class="sd">    ...     &#39;RandomForestClassifier&#39;: {</span>
<span class="sd">    ...         &#39;n_estimators&#39;: [100, 200],</span>
<span class="sd">    ...         &#39;max_depth&#39;: [None, 10, 20]</span>
<span class="sd">    ...     }</span>
<span class="sd">    ... }</span>
<span class="sd">    &gt;&gt;&gt; priority_tuners = [&#39;random&#39;, &#39;grid&#39;]</span>
<span class="sd">    &gt;&gt;&gt; ml_models_with_custom_tuning = predict_ml(</span>
<span class="sd">    ...     df,</span>
<span class="sd">    ...     x_cols=x_cols,</span>
<span class="sd">    ...     y_col=y_col,</span>
<span class="sd">    ...     priority_metrics=priority_metrics,</span>
<span class="sd">    ...     refit_metric=&#39;r2&#39;,</span>
<span class="sd">    ...     priority_tuners=priority_tuners,</span>
<span class="sd">    ...     custom_param_grids=custom_grids,</span>
<span class="sd">    ...     verbose=2</span>
<span class="sd">    ... )</span>

<span class="sd">    **Advanced Machine Learning Pipeline Using Bayesian Optimization:**</span>

<span class="sd">    &gt;&gt;&gt; priority_tuners = [&#39;bayesian&#39;]</span>
<span class="sd">    &gt;&gt;&gt; n_iter_bayesian = 50</span>
<span class="sd">    &gt;&gt;&gt; ml_models_bayesian = predict_ml(</span>
<span class="sd">    ...     df,</span>
<span class="sd">    ...     x_cols=x_cols,</span>
<span class="sd">    ...     y_col=y_col,</span>
<span class="sd">    ...     priority_metrics=priority_metrics,</span>
<span class="sd">    ...     refit_metric=&#39;r2&#39;,</span>
<span class="sd">    ...     priority_tuners=priority_tuners,</span>
<span class="sd">    ...     n_iter_bayesian=n_iter_bayesian,</span>
<span class="sd">    ...     verbose=2</span>
<span class="sd">    ... )</span>


<span class="sd">    Inference Pipeline Examples</span>
<span class="sd">    ^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>

<span class="sd">    **Simple Inference Example Using a Statistical Model:**</span>

<span class="sd">    Find the relationship between `Salary` and `Age` using Ordinary Least Squares (OLS) regression:</span>

<span class="sd">    &gt;&gt;&gt; formula = &#39;Salary ~ Age&#39;</span>
<span class="sd">    &gt;&gt;&gt; inference_result_ols = predict_ml(df, formula=formula, verbose=2)</span>

<span class="sd">    **Using a Categorical Predictor with OLS:**</span>

<span class="sd">    Incorporate a categorical variable (`Department`) in the model to examine its effect on `Salary`:</span>

<span class="sd">    &gt;&gt;&gt; formula = &#39;Salary ~ Age + C(Department)&#39;</span>
<span class="sd">    &gt;&gt;&gt; inference_result_ols_categorical = predict_ml(df, formula=formula, verbose=2)</span>

<span class="sd">    **Advanced Inference with Multiple Models and Specific Metrics:**</span>

<span class="sd">    Compare multiple regression models focusing on their fit statistics:</span>

<span class="sd">    &gt;&gt;&gt; priority_models = [&#39;OLS&#39;, &#39;WLS&#39;, &#39;GLS&#39;]</span>
<span class="sd">    &gt;&gt;&gt; advanced_inference_models = predict_ml(</span>
<span class="sd">    ...     df,</span>
<span class="sd">    ...     formula=formula,</span>
<span class="sd">    ...     priority_models=priority_models,</span>
<span class="sd">    ...     verbose=2</span>
<span class="sd">    ... )</span>

<span class="sd">    **Inference with Robust Regression Model:**</span>

<span class="sd">    Utilize Robust Linear Models (RLM) to mitigate the influence of outliers:</span>

<span class="sd">    &gt;&gt;&gt; formula = &#39;Salary ~ Age + C(Department)&#39;</span>
<span class="sd">    &gt;&gt;&gt; robust_inference_result = predict_ml(</span>
<span class="sd">    ...     df,</span>
<span class="sd">    ...     formula=formula,</span>
<span class="sd">    ...     priority_models=[&#39;RLM&#39;],</span>
<span class="sd">    ...     verbose=2</span>
<span class="sd">    ... )</span>

<span class="sd">    **Mixed Linear Model for Hierarchical or Longitudinal Data:**</span>

<span class="sd">    Apply a Mixed Linear Model (MixedLM) if the data structure involves nested or grouped observations:</span>

<span class="sd">    &gt;&gt;&gt; formula = &#39;Salary ~ Age + C(Department) + (1|Employment Date)&#39;</span>
<span class="sd">    &gt;&gt;&gt; mixedlm_inference_result = predict_ml(</span>
<span class="sd">    ...     df,</span>
<span class="sd">    ...     formula=formula,</span>
<span class="sd">    ...     priority_models=[&#39;MixedLM&#39;],</span>
<span class="sd">    ...     verbose=2</span>
<span class="sd">    ... )</span>


<span class="sd">    Notes:</span>
<span class="sd">    ------</span>

<span class="sd">    Pipelines Explained</span>
<span class="sd">    ^^^^^^^^^^^^^^^^^^^</span>

<span class="sd">    Machine Learning Pipeline</span>
<span class="sd">    +++++++++++++++++++++++++</span>
<span class="sd">        1. **Data Preprocessing (optional):** optionally prepares a dataset for machine learning by handling numerical, categorical, text, and datetime data.</span>
<span class="sd">            - It supports flexible imputation, scaling, encoding, and vectorization methods to cater to a wide range of preprocessing needs.</span>
<span class="sd">            - The function automatically splits the data into training and test sets and applies the preprocessing steps defined by the user.</span>
<span class="sd">            - It accommodates custom preprocessing steps for various data types, enhancing flexibility and control over the preprocessing pipeline.</span>

<span class="sd">        2. **Evaluation of Untuned Models:** leverages a composite score for model evaluation, which synthesizes scores across multiple metrics, weighted by the specified priorities. This method enables a holistic and nuanced model comparison, taking into account the multidimensional aspects of model performance.</span>
<span class="sd">            - **Priority Metrics:** Assigning weights (default: 5 for prioritized metrics, 1 for others) allows users to emphasize metrics they find most relevant, affecting the composite score calculation.</span>
<span class="sd">            - **Composite Score:** Calculated as a weighted average of metric scores, normalized by the total weight. This score serves as a basis for ranking models. The formula for the composite score is given by:</span>

<span class="sd">            .. math::</span>

<span class="sd">                C = \frac{\sum_{m \in M} (w_m \cdot \text{adj}(s_m))}{\sum_{m \in M} w_m}</span>

<span class="sd">            Where:</span>
<span class="sd">                - :math:`\text{adj}(s_m)` is the score adjustment function, ensuring a consistent interpretation across metrics. Metrics for which lower values are traditionally better (e.g., RMSE, MAE) are inverted or negated prior to weight application, aligning all metrics to the &quot;higher is better&quot; principle for score calculation.</span>
<span class="sd">                - :math:`w_m` represents the weight of metric :math:`m`.</span>
<span class="sd">                - :math:`M` is the set of all metrics considered in the evaluation.</span>

<span class="sd">        3. **Model Tuning:** Uses top N untuned models to tune. Systematically applies grid search, random search, or Bayesian optimization to explore the hyperparameter space of given models. It supports customization of the tuning process through various parameters and outputs the best found configurations.</span>


<span class="sd">    Statistical Inference Pipeline</span>
<span class="sd">    ++++++++++++++++++++++++++++++</span>
<span class="sd">        1. **Determination of Task Type:** First, the function identifies whether the analysis involves regression or classification. This categorization is based on the datatype of the target variable specified in the formula:</span>
<span class="sd">            - **Regression:** Applied if the target variable is numerical.</span>
<span class="sd">            - **Classification:** Applied if the target variable is categorical.</span>

<span class="sd">        2. **Model Selection:** Based on the task type determined in the previous step, the function selects from a pre-defined set of models suitable for either regression or classification:</span>
<span class="sd">            - Models and their respective functions are predefined in the `models_classification_inference` or `models_regression_inference` dictionaries, depending on whether the task is classification or regression.</span>
<span class="sd">            - The user has the option to limit the evaluation to a subset of models through the `priority_models` parameter, enhancing focus and computational efficiency.</span>

<span class="sd">        3. **Model Evaluation:** Each selected model is fitted to the data using the formula provided:</span>
<span class="sd">            - The function iterates over each model, passing any user-defined keyword arguments specific to that model using the `model_kwargs` dictionary. This allows for customized model configurations.</span>
<span class="sd">            - Models are fitted using their respective statistical functions from the statsmodels API, adhering to the specifications in the formula.</span>

<span class="sd">        4. **Metrics Calculation:** After fitting, the function evaluates each model using a set of predefined metrics appropriate for the task type:</span>
<span class="sd">            - Certain metrics, particularly those for which a lower value indicates better performance (e.g., AIC, BIC), are adjusted to fit a common scoring scheme where higher values indicate better model performance.</span>

<span class="sd">        5. **Model Ranking and Output:** Finally, the models are ranked based on their performance metrics:</span>
<span class="sd">            - A sorted list of models is generated based on the adjusted metrics, allowing the top-performing models to be identified.</span>
<span class="sd">            - The function returns the top `n_top_models` as specified, including their fitted model objects and performance metrics, facilitating further analysis or validation by the user.</span>
<span class="sd">            - If verbose output is enabled, the function provides detailed summaries of the top models, aiding in interpretive and diagnostic processes.</span>




<span class="sd">    Available Metadata</span>
<span class="sd">    ^^^^^^^^^^^^^^^^^^^</span>

<span class="sd">    Below you can find all of the models, scoring metrics and tuners ``predict_ml()`` is equipped with. We also provide default parameter grids for users who do not wish to provide it.</span>

<span class="sd">    ML-oriented Models</span>
<span class="sd">    ++++++++++++++++++</span>

<span class="sd">        **Classification Models**</span>
<span class="sd">            - **LogisticRegression**: Provides logistic regression for binary classification.</span>
<span class="sd">            - **DecisionTreeClassifier**: Offers decision tree algorithms for classification.</span>
<span class="sd">            - **RandomForestClassifier**: Implements a random forest for classification.</span>
<span class="sd">            - **GradientBoostingClassifier**: Applies gradient boosting techniques for classification.</span>
<span class="sd">            - **SVC**: Support Vector Classifier with enabled probability estimates.</span>
<span class="sd">            - **KNeighborsClassifier**: Utilizes k-nearest neighbors voting classification.</span>

<span class="sd">        **Regression Models**</span>
<span class="sd">            - **LinearRegression**: Ordinary least squares Linear Regression.</span>
<span class="sd">            - **Ridge**: Ridge regression with L2 regularization.</span>
<span class="sd">            - **Lasso**: Lasso regression with L1 regularization.</span>
<span class="sd">            - **DecisionTreeRegressor**: Regression based on decision trees.</span>
<span class="sd">            - **RandomForestRegressor**: Random forest algorithm for regression.</span>
<span class="sd">            - **GradientBoostingRegressor**: Gradient boosting for regression.</span>
<span class="sd">            - **SVR**: Epsilon-Support Vector Regression.</span>
<span class="sd">            - **KNeighborsRegressor**: Regression based on k-nearest neighbors.</span>

<span class="sd">    Inference-oriented Models</span>
<span class="sd">    +++++++++++++++++++++++++</span>

<span class="sd">    These models are specifically used for statistical inference, allowing for detailed statistical analysis.</span>

<span class="sd">        **Classification Inference Models**</span>
<span class="sd">            - **Logit**: Logistic regression for binary classification.</span>
<span class="sd">            - **Probit**: Probit model for binary classification.</span>
<span class="sd">            - **MNLogit**: Multinomial logistic regression for handling multiple categories.</span>
<span class="sd">            - **Poisson**: Poisson model for count data.</span>
<span class="sd">            - **NegativeBinomial**: Negative binomial model for count data with over-dispersion.</span>
<span class="sd">            - **GEE**: Generalized Estimating Equations for longitudinal data.</span>
<span class="sd">            - **NominalGEE**: Generalized Estimating Equations for nominal responses.</span>
<span class="sd">            - **OrdinalGEE**: Generalized Estimating Equations for ordinal responses.</span>

<span class="sd">        **Regression Inference Models**</span>
<span class="sd">            - **OLS**: Ordinary Least Squares for linear regression.</span>
<span class="sd">            - **WLS**: Weighted Least Squares for cases with non-constant variance.</span>
<span class="sd">            - **GLS**: Generalized Least Squares for regression with correlated errors.</span>
<span class="sd">            - **RLM**: Robust Linear Models for regression with outliers.</span>
<span class="sd">            - **QuantReg**: Quantile Regression for modeling different quantiles.</span>
<span class="sd">            - **GLSAR**: GLS with autoregressive error model.</span>
<span class="sd">            - **MixedLM**: Mixed Linear Model for hierarchical or longitudinal data.</span>
<span class="sd">            - **PHReg**: Proportional Hazards model for survival analysis.</span>

<span class="sd">    Scoring Metrics</span>
<span class="sd">    +++++++++++++++</span>

<span class="sd">        **Classification Scoring Metrics**</span>
<span class="sd">            - **Accuracy, Balanced Accuracy**: Measures overall and balanced accuracy.</span>
<span class="sd">            - **Average Precision, F1 Score Variants**: Assesses precision-recall balance.</span>
<span class="sd">            - **Negative Log Loss**: Negative log-likelihood of the classifier.</span>
<span class="sd">            - **Precision, Recall, Jaccard Index**: Evaluates the positive identified samples.</span>
<span class="sd">            - **ROC AUC**: Area Under the ROC Curve for model discrimination capability.</span>

<span class="sd">        **Regression Scoring Metrics**</span>
<span class="sd">            - **Explained Variance, MAE, MSE, RMSE, MSLE, MedAE**: Measures of error and variance explained by the model.</span>
<span class="sd">            - **R2, MPD, MGD, MAPE**: Metrics for accuracy and prediction deviation.</span>

<span class="sd">    Model Tuners</span>
<span class="sd">    ++++++++++++</span>
<span class="sd">        - **GridSearchCV**: Exhaustive search over specified parameter values.</span>
<span class="sd">        - **RandomizedSearchCV**: Randomized search on hyper parameters.</span>
<span class="sd">        - **BayesSearchCV**: Bayesian approach to hyperparameter optimization.</span>

<span class="sd">    Default Parameter Grids</span>
<span class="sd">    +++++++++++++++++++++++</span>

<span class="sd">    We provide default parameter grids for users who do not wish to provide it. Feel free to use the ones below as a template for your own to use with ``predict_ml(custom_param_grids=..)``</span>

<span class="sd">    **Parameter Grid for Classification Model Tuning**</span>

<span class="sd">    &gt;&gt;&gt; default_param_grids_classification = {</span>
<span class="sd">    ...     &#39;LogisticRegression&#39;: {</span>
<span class="sd">    ...         &#39;C&#39;: [0.1, 1, 10, 100],</span>
<span class="sd">    ...         &#39;penalty&#39;: [&#39;l2&#39;],</span>
<span class="sd">    ...         &#39;solver&#39;: [&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;DecisionTreeClassifier&#39;: {</span>
<span class="sd">    ...         &#39;max_depth&#39;: [None, 10, 20, 30, 40, 50],</span>
<span class="sd">    ...         &#39;min_samples_split&#39;: [2, 5, 10],</span>
<span class="sd">    ...         &#39;min_samples_leaf&#39;: [1, 2, 4]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;RandomForestClassifier&#39;: {</span>
<span class="sd">    ...         &#39;n_estimators&#39;: [100, 200, 300, 400],</span>
<span class="sd">    ...         &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;],</span>
<span class="sd">    ...         &#39;max_depth&#39;: [None, 10, 20, 30, 40],</span>
<span class="sd">    ...         &#39;min_samples_split&#39;: [2, 5, 10],</span>
<span class="sd">    ...         &#39;min_samples_leaf&#39;: [1, 2, 4]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;GradientBoostingClassifier&#39;: {</span>
<span class="sd">    ...         &#39;n_estimators&#39;: [100, 200, 300],</span>
<span class="sd">    ...         &#39;learning_rate&#39;: [0.01, 0.1, 0.2, 0.5],</span>
<span class="sd">    ...         &#39;max_depth&#39;: [3, 5, 7, 9]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;SVC&#39;: {</span>
<span class="sd">    ...         &#39;C&#39;: [0.1, 1, 10, 100, 1000],</span>
<span class="sd">    ...         &#39;kernel&#39;: [&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;],</span>
<span class="sd">    ...         &#39;gamma&#39;: [&#39;scale&#39;, &#39;auto&#39;]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;KNeighborsClassifier&#39;: {</span>
<span class="sd">    ...         &#39;n_neighbors&#39;: [3, 5, 7, 9],</span>
<span class="sd">    ...         &#39;weights&#39;: [&#39;uniform&#39;, &#39;distance&#39;],</span>
<span class="sd">    ...         &#39;algorithm&#39;: [&#39;auto&#39;, &#39;ball_tree&#39;, &#39;kd_tree&#39;, &#39;brute&#39;]</span>
<span class="sd">    ...     }</span>
<span class="sd">    ... }</span>


<span class="sd">    **Parameter Grid for Regression Model Tuning**</span>

<span class="sd">    &gt;&gt;&gt; default_param_grids_regression = {</span>
<span class="sd">    ...     &#39;LinearRegression&#39;: {</span>
<span class="sd">    ...         # Linear Regression usually does not need hyperparameter tuning except for regularization</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;Ridge&#39;: {</span>
<span class="sd">    ...         &#39;alpha&#39;: [0.1, 1.0, 10.0, 100.0],</span>
<span class="sd">    ...         &#39;solver&#39;: [&#39;auto&#39;, &#39;svd&#39;, &#39;cholesky&#39;, &#39;lsqr&#39;, &#39;sparse_cg&#39;, &#39;sag&#39;, &#39;saga&#39;]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;Lasso&#39;: {</span>
<span class="sd">    ...         &#39;alpha&#39;: [0.1, 1.0, 10.0, 100.0],</span>
<span class="sd">    ...         &#39;selection&#39;: [&#39;cyclic&#39;, &#39;random&#39;]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;DecisionTreeRegressor&#39;: {</span>
<span class="sd">    ...         &#39;max_depth&#39;: [None, 10, 20, 30, 40, 50],</span>
<span class="sd">    ...         &#39;min_samples_split&#39;: [2, 5, 10],</span>
<span class="sd">    ...         &#39;min_samples_leaf&#39;: [1, 2, 4]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;RandomForestRegressor&#39;: {</span>
<span class="sd">    ...         &#39;n_estimators&#39;: [100, 200, 300, 400],</span>
<span class="sd">    ...         &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;],</span>
<span class="sd">    ...         &#39;max_depth&#39;: [None, 10, 20, 30, 40],</span>
<span class="sd">    ...         &#39;min_samples_split&#39;: [2, 5, 10],</span>
<span class="sd">    ...         &#39;min_samples_leaf&#39;: [1, 2, 4]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;GradientBoostingRegressor&#39;: {</span>
<span class="sd">    ...         &#39;n_estimators&#39;: [100, 200, 300],</span>
<span class="sd">    ...         &#39;learning_rate&#39;: [0.01, 0.1, 0.2, 0.5],</span>
<span class="sd">    ...         &#39;max_depth&#39;: [3, 5, 7, 9]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;SVR&#39;: {</span>
<span class="sd">    ...         &#39;C&#39;: [0.1, 1, 10, 100, 1000],</span>
<span class="sd">    ...         &#39;kernel&#39;: [&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;],</span>
<span class="sd">    ...         &#39;gamma&#39;: [&#39;scale&#39;, &#39;auto&#39;]</span>
<span class="sd">    ...     },</span>
<span class="sd">    ...     &#39;KNeighborsRegressor&#39;: {</span>
<span class="sd">    ...         &#39;n_neighbors&#39;: [3, 5, 7, 9],</span>
<span class="sd">    ...         &#39;weights&#39;: [&#39;uniform&#39;, &#39;distance&#39;],</span>
<span class="sd">    ...         &#39;algorithm&#39;: [&#39;auto&#39;, &#39;ball_tree&#39;, &#39;kd_tree&#39;, &#39;brute&#39;]</span>
<span class="sd">    ...     }</span>
<span class="sd">    ... }</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">datetime_transformer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">datetime_transformer</span> <span class="o">=</span> <span class="n">FunctionTransformer</span><span class="p">(</span><span class="n">datetime_feature_extractor</span><span class="p">,</span> <span class="n">validate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">formula</span> <span class="ow">and</span> <span class="n">df</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Inference pipeline</span>
        <span class="k">return</span> <span class="n">model_recommendation_core_inference</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">formula</span><span class="p">,</span> <span class="n">priority_models</span><span class="o">=</span><span class="n">priority_models</span><span class="p">,</span> <span class="n">n_top_models</span><span class="o">=</span><span class="n">n_top_models</span><span class="p">,</span>
                                                   <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">x_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y_col</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">df</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># ML pipeline</span>
        <span class="n">x_train_processed</span><span class="p">,</span> <span class="n">x_test_processed</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">task_type</span> <span class="o">=</span> <span class="n">data_preprocessing_core</span><span class="p">(</span>
            <span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
            <span class="n">x_cols</span><span class="o">=</span><span class="n">x_cols</span><span class="p">,</span>
            <span class="n">y_col</span><span class="o">=</span><span class="n">y_col</span><span class="p">,</span>
            <span class="n">data_state</span><span class="o">=</span><span class="n">data_state</span><span class="p">,</span>
            <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">numeric_imputer</span><span class="o">=</span><span class="n">numeric_imputer</span><span class="p">,</span>
            <span class="n">numeric_scaler</span><span class="o">=</span><span class="n">numeric_scaler</span><span class="p">,</span>
            <span class="n">categorical_imputer</span><span class="o">=</span><span class="n">categorical_imputer</span><span class="p">,</span>
            <span class="n">categorical_encoder</span><span class="o">=</span><span class="n">categorical_encoder</span><span class="p">,</span>
            <span class="n">text_vectorizer</span><span class="o">=</span><span class="n">text_vectorizer</span><span class="p">,</span>
            <span class="n">datetime_transformer</span><span class="o">=</span><span class="n">datetime_transformer</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="p">)</span>

        <span class="n">recommended_models</span> <span class="o">=</span> <span class="n">model_recommendation_core</span><span class="p">(</span>
            <span class="n">x_train</span><span class="o">=</span><span class="n">x_train_processed</span><span class="p">,</span>
            <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
            <span class="n">task_type</span><span class="o">=</span><span class="n">task_type</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
            <span class="n">priority_metrics</span><span class="o">=</span><span class="n">priority_metrics</span><span class="p">,</span>
            <span class="n">n_top_models</span><span class="o">=</span><span class="n">n_top_models</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="p">)</span>

        <span class="n">tuned_models</span> <span class="o">=</span> <span class="n">model_tuning_core</span><span class="p">(</span>
            <span class="n">x_train</span><span class="o">=</span><span class="n">x_train_processed</span><span class="p">,</span>
            <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
            <span class="n">task_type</span><span class="o">=</span><span class="n">task_type</span><span class="p">,</span>
            <span class="n">models</span><span class="o">=</span><span class="n">recommended_models</span><span class="p">,</span>
            <span class="n">priority_metrics</span><span class="o">=</span><span class="n">priority_metrics</span><span class="p">,</span>
            <span class="n">refit_metric</span><span class="o">=</span><span class="n">refit_metric</span><span class="p">,</span>
            <span class="n">priority_tuners</span><span class="o">=</span><span class="n">priority_tuners</span><span class="p">,</span>
            <span class="n">custom_param_grids</span><span class="o">=</span><span class="n">custom_param_grids</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
            <span class="n">n_iter_random</span><span class="o">=</span><span class="n">n_iter_random</span><span class="p">,</span>
            <span class="n">n_iter_bayesian</span><span class="o">=</span><span class="n">n_iter_bayesian</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">tuned_models</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid input: Either provide a formula for inference or x_cols and y_col for machine learning.&quot;</span><span class="p">)</span></div>

</pre></div>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, George Dreemer
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer no-toc">
      
      
      
    </aside>
  </div>
</div><script src="../../../_static/documentation_options.js?v=8d563738"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/scripts/furo.js?v=4e2eecee"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    </body>
</html>